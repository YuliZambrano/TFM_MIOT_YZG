{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import unicodedata\n",
    "from urllib.parse import quote\n",
    "\n",
    "# ============================================================\n",
    "# PUNTO 1 (METADATOS) - EJEMPLO PORTAL: datos.gob.es (Portal Nacional)\n",
    "# Fuente: APIDATA (Linked Data API) + (opcional) SPARQL para labels\n",
    "# ============================================================\n",
    "\n",
    "PORTAL = \"datos.gob.es (Portal Nacional)\"\n",
    "API_TYPE = \"APIDATA+SPARQL\"  #TIPO DE API\n",
    " \n",
    "APIDATA_BASE = \"https://datos.gob.es/apidata/catalog/dataset.json\" #URL DE API\n",
    "SPARQL_ENDPOINT = \"https://datos.gob.es/virtuoso/sparql\"           #URL ENDPOINT DE API   \n",
    "\n",
    "TIMEOUT = 60\n",
    "SLEEP = 0.05\n",
    "MAX_DATASETS = 500      # PUEDE CAMBIAR EL LIMITE DE DATASET\n",
    "PAGE_SIZE = 500         # se ajusta a max 50 por especificación APIDATA\n",
    "\n",
    "MIN_AGE_MONTHS = 12             # PUEDE CAMBIAR EL CRITERIO DE EDAD DEL DATASET\n",
    "\n",
    "UA_HEADERS = {\n",
    "    \"User-Agent\": \"TFM-IIP-Metadata-Extractor/1.0\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "OPEN_FORMATS = {\"CSV\",\"JSON\",\"GEOJSON\",\"XML\",\"RDF\",\"TTL\",\"TURTLE\",\"N-TRIPLES\",\"NT\",\"JSON-LD\",\"JSONLD\"}\n",
    "SEMANTIC_FORMATS = {\"RDF\",\"TTL\",\"TURTLE\",\"N-TRIPLES\",\"NT\",\"JSON-LD\",\"JSONLD\"}\n",
    "\n",
    "DATA_DICT_PATTERNS = [\n",
    "    r\"diccionario de datos\", r\"data dictionary\", r\"schema\", r\"esquema\",\n",
    "    r\"documentaci[oó]n\", r\"metadatos\", r\"data model\", r\"glosario\"\n",
    "]\n",
    "\n",
    " # PUEDE CAMBIAR LOS CRITERIOS DE CATEGORIAS\n",
    "CATEGORIAS_KEYWORDS = {\n",
    "    \"Transporte y Movilidad\": [\n",
    "        \"transporte\", \"movilidad\", \"trafico\", \"tráfico\", \"carreteras\",\n",
    "        \"vehiculos\", \"vehículos\", \"autobuses\", \"metro\", \"ciclistas\",\n",
    "        \"aparcamientos\", \"taxis\", \"vialidad\"\n",
    "    ],\n",
    "    \"Ciencia y Tecnología\": [\n",
    "        \"ciencia\", \"tecnologia\", \"tecnología\", \"innovacion\", \"innovación\",\n",
    "        \"investigacion\", \"investigación\", \"i+d\", \"proyectos\",\n",
    "        \"desarrollo\", \"sistemas\", \"tecnologías\"\n",
    "    ],\n",
    "    \"Salud\": [\n",
    "        \"salud\", \"sanidad\", \"hospitales\", \"urgencias\", \"epidemiologia\",\n",
    "        \"epidemiología\", \"asistencia sanitaria\", \"covid\", \"enfermedades\",\n",
    "        \"vacunacion\", \"vacunación\", \"salud pública\", \"farmacias\"\n",
    "    ],\n",
    "    \"Educación\": [\n",
    "        \"educacion\", \"educación\", \"formacion\", \"formación\", \"universidad\",\n",
    "        \"universidades\", \"colegios\", \"institutos\", \"centros educativos\",\n",
    "        \"profesorado\", \"alumnado\", \"matriculas\", \"matrículas\"\n",
    "    ],\n",
    "    \"Datos Geográficos y Medioambientales\": [\n",
    "        \"medio ambiente\", \"medio-ambiente\", \"geografia\", \"geografía\",\n",
    "        \"cartografia\", \"cartografía\", \"clima\", \"meteorologia\", \"meteorología\",\n",
    "        \"contaminacion\", \"contaminación\", \"biodiversidad\", \"parques\",\n",
    "        \"rios\", \"ríos\", \"fauna\", \"flora\", \"ecologia\", \"ecología\"\n",
    "    ],\n",
    "    \"Datos Demográficos y Estadísticos\": [\n",
    "        \"demografia\", \"demografía\", \"estadistica\", \"estadística\",\n",
    "        \"poblacion\", \"población\", \"municipios\", \"censos\",\n",
    "        \"indicadores\", \"series temporales\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "DOI_REGEX = r\"(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers generales\n",
    "# ------------------------------------------------------------\n",
    "def safe_get_json(url, params=None, max_retries=5, backoff_base=1.4):\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=UA_HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                time.sleep(backoff_base ** attempt)\n",
    "                last_err = RuntimeError(f\"HTTP {r.status_code} temporal: {r.url}\")\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(backoff_base ** attempt)\n",
    "    raise last_err\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(c for c in s if unicodedata.category(c) != \"Mn\")\n",
    "    return s\n",
    "\n",
    "def parse_dt(s: str):\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        s2 = str(s).strip().replace(\"Z\", \"+00:00\")\n",
    "        dt = datetime.fromisoformat(s2)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def meets_age(dt_utc):\n",
    "    if not dt_utc:\n",
    "        return True\n",
    "    limite = datetime.now(timezone.utc) - timedelta(days=MIN_AGE_MONTHS * 30.44)\n",
    "    return dt_utc <= limite\n",
    "\n",
    "def contains_any(text: str, patterns) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    t = str(text).lower()\n",
    "    return any(re.search(p, t) for p in patterns)\n",
    "\n",
    "def pick_lang_value(val, preferred=\"es\"):\n",
    "    \"\"\"\n",
    "    APIDATA suele devolver title/description como:\n",
    "    [{\"_value\":\"...\",\"_lang\":\"es\"}, ...]\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    if isinstance(val, str):\n",
    "        return val\n",
    "    if isinstance(val, list):\n",
    "        # preferido\n",
    "        for it in val:\n",
    "            if isinstance(it, dict) and it.get(\"_lang\") == preferred and it.get(\"_value\"):\n",
    "                return str(it[\"_value\"])\n",
    "        # fallback primer valor con _value\n",
    "        for it in val:\n",
    "            if isinstance(it, dict) and it.get(\"_value\"):\n",
    "                return str(it[\"_value\"])\n",
    "        # fallback str del primer elemento\n",
    "        if val:\n",
    "            return str(val[0])\n",
    "    if isinstance(val, dict):\n",
    "        return str(val.get(\"_value\") or val.get(\"value\") or \"\")\n",
    "    return str(val)\n",
    "\n",
    "def normalize_format(fmt_value: str) -> str:\n",
    "    \"\"\"\n",
    "    Convierte MIME/strings a formatos tipo CSV/JSON/XML...\n",
    "    Ej: 'text/csv' -> 'CSV', 'application/json' -> 'JSON'\n",
    "    \"\"\"\n",
    "    if not fmt_value:\n",
    "        return \"\"\n",
    "    s = fmt_value.strip().lower()\n",
    "\n",
    "    mapping = {\n",
    "        \"text/csv\": \"CSV\",\n",
    "        \"application/csv\": \"CSV\",\n",
    "        \"application/json\": \"JSON\",\n",
    "        \"application/geo+json\": \"GEOJSON\",\n",
    "        \"application/geojson\": \"GEOJSON\",\n",
    "        \"application/xml\": \"XML\",\n",
    "        \"text/xml\": \"XML\",\n",
    "        \"application/rdf+xml\": \"RDF\",\n",
    "        \"text/turtle\": \"TTL\",\n",
    "        \"application/x-turtle\": \"TTL\",\n",
    "        \"application/ld+json\": \"JSON-LD\",\n",
    "        \"application/vnd.ms-excel\": \"XLS\",\n",
    "        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\": \"XLSX\",\n",
    "    }\n",
    "    if s in mapping:\n",
    "        return mapping[s]\n",
    "\n",
    "    # heurísticas por substring\n",
    "    if \"csv\" in s:\n",
    "        return \"CSV\"\n",
    "    if \"json\" in s and \"ld\" in s:\n",
    "        return \"JSON-LD\"\n",
    "    if \"json\" in s:\n",
    "        return \"JSON\"\n",
    "    if \"xml\" in s:\n",
    "        return \"XML\"\n",
    "    if \"turtle\" in s or \"ttl\" in s:\n",
    "        return \"TTL\"\n",
    "    if \"rdf\" in s:\n",
    "        return \"RDF\"\n",
    "    if \"xlsx\" in s:\n",
    "        return \"XLSX\"\n",
    "\n",
    "    # fallback: última parte de mime\n",
    "    if \"/\" in s:\n",
    "        tail = s.split(\"/\")[-1].upper()\n",
    "        return tail[:20]\n",
    "    return s.upper()[:20]\n",
    "\n",
    "def extract_doi_from_text(*parts) -> str:\n",
    "    blob = \" \".join([p for p in parts if p])\n",
    "    m = re.search(DOI_REGEX, blob, flags=re.I)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def classify_category_from_blob(title, description, keywords, theme_uri) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Clasificación \"real\" para este portal: no hay groups/tags CKAN,\n",
    "    así que usamos:\n",
    "    - keywords (APIDATA 'keyword')\n",
    "    - theme URI (APIDATA 'theme')\n",
    "    - texto (title+description)\n",
    "    \"\"\"\n",
    "    kw_blob = \"\"\n",
    "    if isinstance(keywords, list):\n",
    "        kw_blob = \" \".join([pick_lang_value(k) for k in keywords])\n",
    "    elif keywords:\n",
    "        kw_blob = str(keywords)\n",
    "\n",
    "    theme_txt = \"\"\n",
    "    if theme_uri:\n",
    "        theme_txt = str(theme_uri).rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    blob = normalize_text(f\"{title} {description} {kw_blob} {theme_txt}\")\n",
    "\n",
    "    for cat, kws in CATEGORIAS_KEYWORDS.items():\n",
    "        if any(normalize_text(kw) in blob for kw in kws):\n",
    "            # source \"keyword/theme/text\" (no CKAN groups)\n",
    "            return cat, \"keyword/theme/text\"\n",
    "\n",
    "    return \"No definido\", \"none\"\n",
    "\n",
    "def detect_controlled_vocab_from_apidata(theme_uri, keywords):\n",
    "    # En datos.gob.es, theme suele venir de taxonomía -> vocab controlado\n",
    "    if theme_uri:\n",
    "        return True, \"theme\"\n",
    "    if keywords:\n",
    "        return True, \"keyword\"\n",
    "    return False, \"none\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# APIDATA fetch\n",
    "# ------------------------------------------------------------\n",
    "def apidata_fetch_page(page: int, page_size: int):\n",
    "    # APIDATA doc: _pageSize max 50 (ajustamos para no romper) :contentReference[oaicite:3]{index=3}\n",
    "    ps = min(int(page_size), 50)\n",
    "    params = {\n",
    "        \"_page\": page,\n",
    "        \"_pageSize\": ps,\n",
    "        \"_metadata\": \"all\",\n",
    "        \"_sort\": \"title\",\n",
    "    }\n",
    "    js = safe_get_json(APIDATA_BASE, params=params)\n",
    "    result = (js or {}).get(\"result\") or {}\n",
    "    items = result.get(\"items\") or []\n",
    "    return items\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SPARQL: resolver labels de URIs (publisher principalmente)\n",
    "# ------------------------------------------------------------\n",
    "def sparql_select(query: str):\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"application/sparql-results+json\"\n",
    "    }\n",
    "    return safe_get_json(SPARQL_ENDPOINT, params=params)\n",
    "\n",
    "def sparql_resolve_labels(uris: list[str], prefer_lang=\"es\") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Devuelve dict uri -> label, en lotes para performance.\n",
    "    \"\"\"\n",
    "    uris = [u for u in uris if u and isinstance(u, str) and u.startswith(\"http\")]\n",
    "    if not uris:\n",
    "        return {}\n",
    "\n",
    "    # Lotes chicos\n",
    "    out = {}\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(uris), batch_size):\n",
    "        chunk = uris[i:i+batch_size]\n",
    "        values = \" \".join([f\"<{u}>\" for u in chunk])\n",
    "\n",
    "        q = f\"\"\"\n",
    "        SELECT ?s (COALESCE(?lbl_es, ?lbl_any, STR(?s)) AS ?label) WHERE {{\n",
    "          VALUES ?s {{ {values} }}\n",
    "          OPTIONAL {{ ?s <http://www.w3.org/2000/01/rdf-schema#label> ?lbl_es .\n",
    "                     FILTER(lang(?lbl_es) = \"{prefer_lang}\") }}\n",
    "          OPTIONAL {{ ?s <http://www.w3.org/2000/01/rdf-schema#label> ?lbl_any . }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            js = sparql_select(q)\n",
    "            bindings = js.get(\"results\", {}).get(\"bindings\", []) or []\n",
    "            for b in bindings:\n",
    "                s = b.get(\"s\", {}).get(\"value\")\n",
    "                lab = b.get(\"label\", {}).get(\"value\")\n",
    "                if s:\n",
    "                    out[s] = lab or s\n",
    "        except Exception:\n",
    "            # si falla SPARQL, no detenemos pipeline\n",
    "            continue\n",
    "\n",
    "        time.sleep(SLEEP)\n",
    "    return out\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Evaluación dataset -> fila (APIDATA item)\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_dataset_apidata(item: dict, publisher_label_map: dict[str, str]):\n",
    "    dataset_uri = item.get(\"_about\") or item.get(\"identifier\") or \"\"\n",
    "    identifier = dataset_uri\n",
    "\n",
    "    title = pick_lang_value(item.get(\"title\"))\n",
    "    description = pick_lang_value(item.get(\"description\"))\n",
    "\n",
    "    issued_raw = item.get(\"issued\") or \"\"\n",
    "    modified_raw = item.get(\"modified\") or \"\"\n",
    "\n",
    "    issued_dt = parse_dt(issued_raw)\n",
    "    meets_age_flag = 1 if meets_age(issued_dt) else 0\n",
    "\n",
    "    # publisher puede venir como URI\n",
    "    publisher_uri = item.get(\"publisher\") or \"\"\n",
    "    publisher = publisher_label_map.get(publisher_uri, publisher_uri)\n",
    "\n",
    "    # licencia (puede ser URI o texto)\n",
    "    license_val = item.get(\"license\") or \"\"\n",
    "    license_text = pick_lang_value(license_val) if isinstance(license_val, (list, dict)) else str(license_val)\n",
    "    license_present = 1 if str(license_text).strip() else 0\n",
    "    license_open = 1 if (\"creative commons\" in license_text.lower() or re.search(r\"\\bcc\\s*by\\b\", license_text.lower())) else 0\n",
    "\n",
    "    # theme / keyword\n",
    "    theme_uri = item.get(\"theme\") or \"\"\n",
    "    keywords = item.get(\"keyword\") or []\n",
    "\n",
    "    category, category_source = classify_category_from_blob(title, description, keywords, theme_uri)\n",
    "    uses_cv, cv_source = detect_controlled_vocab_from_apidata(theme_uri, keywords)\n",
    "\n",
    "    # distributions\n",
    "    distributions = item.get(\"distribution\") or []\n",
    "    download_urls = []\n",
    "    formats = []\n",
    "    doc_hint = []\n",
    "\n",
    "    for d in distributions:\n",
    "        if not isinstance(d, dict):\n",
    "            continue\n",
    "        acc = d.get(\"accessURL\") or \"\"\n",
    "        dl = d.get(\"downloadURL\") or \"\"\n",
    "        if dl:\n",
    "            download_urls.append(dl)\n",
    "        elif acc:\n",
    "            download_urls.append(acc)\n",
    "\n",
    "        fmt_obj = d.get(\"format\")\n",
    "        fmt_val = \"\"\n",
    "        if isinstance(fmt_obj, dict):\n",
    "            fmt_val = fmt_obj.get(\"value\") or fmt_obj.get(\"_value\") or \"\"\n",
    "        elif isinstance(fmt_obj, str):\n",
    "            fmt_val = fmt_obj\n",
    "        fmt_norm = normalize_format(fmt_val)\n",
    "        if fmt_norm:\n",
    "            formats.append(fmt_norm)\n",
    "\n",
    "        # hints para diccionario\n",
    "        t = pick_lang_value(d.get(\"title\"))\n",
    "        doc_hint.append(t)\n",
    "        doc_hint.append(d.get(\"accessURL\") or \"\")\n",
    "        doc_hint.append(d.get(\"downloadURL\") or \"\")\n",
    "        doc_hint.append(pick_lang_value(d.get(\"description\")))\n",
    "\n",
    "    formats_unique = sorted(set([f for f in formats if f]))\n",
    "    format_join = \", \".join(formats_unique)\n",
    "    n_formats = len(formats_unique)\n",
    "\n",
    "    has_allowed_format = 1 if any(f in OPEN_FORMATS for f in formats_unique) else 0\n",
    "    has_semantic_serialization = 1 if any(f in SEMANTIC_FORMATS for f in formats_unique) else 0\n",
    "\n",
    "    download_url = download_urls[0] if download_urls else \"\"\n",
    "\n",
    "    # DOI\n",
    "    doi = extract_doi_from_text(title, description, download_url, \" \".join(download_urls))\n",
    "    has_doi = 1 if doi else 0\n",
    "\n",
    "    # data dictionary\n",
    "    has_data_dictionary = 1 if (\n",
    "        contains_any(title + \" \" + description, DATA_DICT_PATTERNS) or\n",
    "        contains_any(\" \".join(doc_hint), DATA_DICT_PATTERNS)\n",
    "    ) else 0\n",
    "\n",
    "    # update frequency (en datos.gob suele venir en accrualPeriodicity)\n",
    "    update_frequency = \"\"\n",
    "    ap = item.get(\"accrualPeriodicity\")\n",
    "    if isinstance(ap, dict):\n",
    "        # a veces trae estructura con \"value\" -> days\n",
    "        update_frequency = str(ap.get(\"_about\") or ap.get(\"value\") or ap)\n",
    "    elif ap:\n",
    "        update_frequency = str(ap)\n",
    "    update_frequency = update_frequency or \"No definido\"\n",
    "    frequency_documented = 1 if update_frequency != \"No definido\" else 0\n",
    "\n",
    "    # performance: no hacemos HEAD/GET masivo\n",
    "    public_access_ok = 1 if download_url else 0\n",
    "\n",
    "    # DCAT en portal nacional: sí\n",
    "    portal_supports_dcat_dcatap = 1\n",
    "    metadata_rdf_available = 1 if dataset_uri else 0\n",
    "\n",
    "     # SE PUEDEN AGREGAR O QUITAR COLUMNAS DE RESULTADOS SIEMPRE QUE LAS DEFINAS\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"portal\": PORTAL,\n",
    "        \"api_type\": API_TYPE,\n",
    "        \"portal_has_api_rest\": 1,\n",
    "        \"portal_supports_dcat_dcatap\": portal_supports_dcat_dcatap,\n",
    "\n",
    "        \"identifier\": identifier,\n",
    "        \"doi\": doi,\n",
    "        \"has_doi\": has_doi,\n",
    "\n",
    "        \"publisher\": publisher,\n",
    "\n",
    "        \"download_url\": download_url,\n",
    "        \"download_urls\": \" | \".join(download_urls),\n",
    "\n",
    "        \"license\": license_text,\n",
    "        \"license_present\": license_present,\n",
    "        \"license_open\": license_open,\n",
    "\n",
    "        \"dataset_id\": dataset_uri,       # en nacional, el id práctico es el URI\n",
    "        \"dataset_uri\": dataset_uri,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "\n",
    "        \"category\": category,\n",
    "        \"category_source\": category_source,\n",
    "\n",
    "        \"uses_controlled_vocab\": 1 if uses_cv else 0,\n",
    "        \"controlled_vocab_source\": cv_source,\n",
    "\n",
    "        \"issued\": issued_raw,\n",
    "        \"modified\": modified_raw,\n",
    "        \"meets_age_criterion\": meets_age_flag,\n",
    "\n",
    "        \"format\": format_join,\n",
    "        \"n_formats\": n_formats,\n",
    "        \"metadata_rdf_available\": metadata_rdf_available,\n",
    "\n",
    "        \"has_allowed_format\": has_allowed_format,\n",
    "        \"has_semantic_serialization\": has_semantic_serialization,\n",
    "        \"has_data_dictionary\": has_data_dictionary,\n",
    "\n",
    "        \"update_frequency\": update_frequency,\n",
    "        \"frequency_documented\": frequency_documented,\n",
    "\n",
    "        \"public_access_ok\": public_access_ok,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 1) Recolectar items (primero rápido)\n",
    "    items_all = []\n",
    "    page = 0\n",
    "    ps = min(int(PAGE_SIZE), 50)  # doc indica max 50 :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "    while True:\n",
    "        items = apidata_fetch_page(page=page, page_size=ps)\n",
    "        if not items:\n",
    "            break\n",
    "        items_all.extend(items)\n",
    "\n",
    "        if MAX_DATASETS and len(items_all) >= MAX_DATASETS:\n",
    "            items_all = items_all[:MAX_DATASETS]\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    print(f\"Items APIDATA recolectados: {len(items_all)} (pageSize efectivo={ps})\")\n",
    "\n",
    "    # 2) Resolver publisher labels por SPARQL (opcional pero útil)\n",
    "    publishers = []\n",
    "    for it in items_all:\n",
    "        p = it.get(\"publisher\")\n",
    "        if p and isinstance(p, str) and p.startswith(\"http\"):\n",
    "            publishers.append(p)\n",
    "    publishers = sorted(set(publishers))\n",
    "\n",
    "    print(f\"Publishers únicos a resolver por SPARQL: {len(publishers)}\")\n",
    "    publisher_label_map = sparql_resolve_labels(publishers, prefer_lang=\"es\")\n",
    "\n",
    "    # 3) Evaluar + filtrar según tus criterios\n",
    "    rows = []\n",
    "    stats = {\"sin_categoria\": 0, \"no_age\": 0, \"no_format\": 0, \"no_download\": 0, \"ok\": 0}\n",
    "\n",
    "    for it in items_all:\n",
    "        reg = evaluate_dataset_apidata(it, publisher_label_map)\n",
    "\n",
    "        # categorías definidas\n",
    "        if reg[\"category\"] == \"No definido\":\n",
    "            stats[\"sin_categoria\"] += 1\n",
    "            continue\n",
    "\n",
    "        # edad mínima\n",
    "        if reg[\"meets_age_criterion\"] != 1:\n",
    "            stats[\"no_age\"] += 1\n",
    "            continue\n",
    "\n",
    "        # formato permitido\n",
    "        if reg[\"has_allowed_format\"] != 1:\n",
    "            stats[\"no_format\"] += 1\n",
    "            continue\n",
    "\n",
    "        # debe tener descarga\n",
    "        if not reg[\"download_url\"]:\n",
    "            stats[\"no_download\"] += 1\n",
    "            continue\n",
    "\n",
    "        rows.append(reg)\n",
    "        stats[\"ok\"] += 1\n",
    "\n",
    "        if stats[\"ok\"] % 50 == 0:\n",
    "            print(f\"OK={stats['ok']} | stats={stats}\")\n",
    "\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "  # SE PUEDEN CAMBIAR LOS NOMBRES DE LOS ARCHIVOS RESULTADOS CSV Y XLSX  \n",
    "    out_csv = \"Punto1_NOMBREPORTAL.csv\"\n",
    "    out_xlsx = \"Punto1_NOMBREPORTAL.xlsx\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    df.to_excel(out_xlsx, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(\"\\n Listo:\", PORTAL)\n",
    "    print(\" Total final:\", len(df))\n",
    "    print(\" Stats:\", stats)\n",
    "    print(\"Archivos:\", out_csv, \"|\", out_xlsx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
