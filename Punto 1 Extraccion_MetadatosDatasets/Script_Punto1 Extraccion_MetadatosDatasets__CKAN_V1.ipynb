{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e62079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Listo: Junta de Andalucía\n",
      "Datasets extraídos: 600 (MAX_DATASETS=600)\n",
      "Archivos: Punto1_Ayto_Andalucia_v1.csv | Punto1_Ayto_Andalucia_v1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "\n",
    "# ============================================================\n",
    "# PUNTO 1 (METADATOS) - PORTAL:  (CKAN)            EJEMPLO PORTAL:\n",
    "# Salida: Excel/CSV con columnas CLAVE (las tuyas) + formatos + categorías + fechas\n",
    "# ============================================================\n",
    "PORTAL = \"Junta de Andalucía\"\n",
    "API_TYPE = \"CKAN\"            #TIPO DE API\n",
    "API_BASE = \"https://www.juntadeandalucia.es/datosabiertos/portal/api/3/action/\"        #URL DE API\n",
    "SITE_BASE = \"https://www.juntadeandalucia.es/datosabiertos/portal\"                   #URL ENDPOINT DE API   \n",
    "\n",
    "DATASET_LANDING_TEMPLATE = f\"{SITE_BASE}/dataset/{{name}}\"\n",
    "RDF_TEMPLATE = f\"{SITE_BASE}/dataset/{{name}}.rdf\"\n",
    "\n",
    "TIMEOUT = 45\n",
    "SLEEP = 0.05\n",
    "ROWS = 200\n",
    "MAX_DATASETS = 600                     # PUEDE CAMBIAR EL LIMITE DE DATASET\n",
    "MIN_AGE_MONTHS = 12                  # PUEDE CAMBIAR EL CRITERIOS DE EDAD DEL DATASET\n",
    "\n",
    "\n",
    "UA_HEADERS = {\n",
    "    \"User-Agent\": \"TFM-IIP-Metadata-Extractor/1.0\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "OPEN_FORMATS = {\"CSV\",\"JSON\",\"GEOJSON\",\"XML\",\"RDF\",\"TTL\",\"TURTLE\",\"N-TRIPLES\",\"NT\",\"JSON-LD\",\"JSONLD\",\"XLSX\"}\n",
    "SEMANTIC_FORMATS = {\"RDF\",\"TTL\",\"TURTLE\",\"N-TRIPLES\",\"NT\",\"JSON-LD\",\"JSONLD\"}\n",
    "\n",
    "DATA_DICT_PATTERNS = [\n",
    "    r\"diccionario de datos\", r\"data dictionary\", r\"schema\", r\"esquema\",\n",
    "    r\"documentaci[oó]n\", r\"metadatos\", r\"data model\", r\"glosario\"\n",
    "]\n",
    "\n",
    " # PUEDE CAMBIAR LOS CRITERIOS DE CATEGORIAS\n",
    "\n",
    "CATEGORIAS_KEYWORDS = {\n",
    "    \"Transporte y Movilidad\": [\n",
    "        \"transporte\", \"movilidad\", \"trafico\", \"tráfico\", \"carreteras\",\n",
    "        \"vehiculos\", \"vehículos\", \"autobuses\", \"metro\", \"ciclistas\",\n",
    "        \"aparcamientos\", \"taxis\", \"vialidad\"\n",
    "    ],\n",
    "    \"Ciencia y Tecnología\": [\n",
    "        \"ciencia\", \"tecnologia\", \"tecnología\", \"innovacion\", \"innovación\",\n",
    "        \"investigacion\", \"investigación\", \"i+d\", \"proyectos\",\n",
    "        \"desarrollo\", \"sistemas\", \"tecnologías\"\n",
    "    ],\n",
    "    \"Salud\": [\n",
    "        \"salud\", \"sanidad\", \"hospitales\", \"urgencias\", \"epidemiologia\",\n",
    "        \"epidemiología\", \"asistencia sanitaria\", \"covid\", \"enfermedades\",\n",
    "        \"vacunacion\", \"vacunación\", \"salud pública\", \"farmacias\"\n",
    "    ],\n",
    "    \"Educación\": [\n",
    "        \"educacion\", \"educación\", \"formacion\", \"formación\", \"universidad\",\n",
    "        \"universidades\", \"colegios\", \"institutos\", \"centros educativos\",\n",
    "        \"profesorado\", \"alumnado\", \"matriculas\", \"matrículas\"\n",
    "    ],\n",
    "    \"Datos Geográficos y Medioambientales\": [\n",
    "        \"medio ambiente\", \"medio-ambiente\", \"geografia\", \"geografía\",\n",
    "        \"cartografia\", \"cartografía\", \"clima\", \"meteorologia\", \"meteorología\",\n",
    "        \"contaminacion\", \"contaminación\", \"biodiversidad\", \"parques\",\n",
    "        \"rios\", \"ríos\", \"fauna\", \"flora\", \"ecologia\", \"ecología\"\n",
    "    ],\n",
    "    \"Datos Demográficos y Estadísticos\": [\n",
    "        \"demografia\", \"demografía\", \"estadistica\", \"estadística\",\n",
    "        \"poblacion\", \"población\", \"municipios\", \"censos\",\n",
    "        \"indicadores\", \"series temporales\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "def safe_get(url, params=None):\n",
    "    r = requests.get(url, params=params, headers=UA_HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    if \"json\" not in ctype:\n",
    "        preview = r.text[:500].replace(\"\\n\", \" \")\n",
    "        raise RuntimeError(f\"Respuesta NO JSON desde {r.url} | Content-Type={ctype} | preview={preview}\")\n",
    "    return r\n",
    "\n",
    "def head_or_get_public(url: str) -> bool:\n",
    "    if not url or not str(url).startswith(\"http\"):\n",
    "        return False\n",
    "    try:\n",
    "        h = requests.head(url, headers=UA_HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "        if h.status_code in (403, 405):\n",
    "            g = requests.get(url, headers=UA_HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "            return 200 <= g.status_code < 400\n",
    "        return 200 <= h.status_code < 400\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(c for c in s if unicodedata.category(c) != \"Mn\")\n",
    "    return s\n",
    "\n",
    "def parse_dt(s: str):\n",
    "    \"\"\"Devuelve datetime UTC-aware o None (evita naive vs aware).\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        s2 = str(s).strip().replace(\"Z\", \"+00:00\")\n",
    "        dt = datetime.fromisoformat(s2)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def meets_age(dt_utc):\n",
    "    if not dt_utc:\n",
    "        return True\n",
    "    limite = datetime.now(timezone.utc) - timedelta(days=MIN_AGE_MONTHS * 30.44)\n",
    "    return dt_utc <= limite\n",
    "\n",
    "def contains_any(text: str, patterns) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    t = str(text).lower()\n",
    "    return any(re.search(p, t) for p in patterns)\n",
    "\n",
    "def extras_to_dict(extras):\n",
    "    d = {}\n",
    "    if isinstance(extras, list):\n",
    "        for it in extras:\n",
    "            if isinstance(it, dict) and \"key\" in it:\n",
    "                d[str(it.get(\"key\"))] = it.get(\"value\")\n",
    "    return d\n",
    "\n",
    "def classify_category_ckan(title, notes, groups, tags):\n",
    "    for g in groups:\n",
    "        gtxt = normalize_text(g.get(\"title\") or g.get(\"name\"))\n",
    "        for cat, kws in CATEGORIAS_KEYWORDS.items():\n",
    "            if any(kw in gtxt for kw in kws):\n",
    "                return cat, \"group\"\n",
    "\n",
    "    for t in tags:\n",
    "        ttxt = normalize_text(t.get(\"name\"))\n",
    "        for cat, kws in CATEGORIAS_KEYWORDS.items():\n",
    "            if any(kw in ttxt for kw in kws):\n",
    "                return cat, \"tag\"\n",
    "\n",
    "    blob = normalize_text(f\"{title} {notes}\")\n",
    "    for cat, kws in CATEGORIAS_KEYWORDS.items():\n",
    "        if any(kw in blob for kw in kws):\n",
    "            return cat, \"text\"\n",
    "\n",
    "    return \"No definido\", \"none\"\n",
    "\n",
    "def detect_controlled_vocab_ckan(groups, tags):\n",
    "    \"\"\"\n",
    "    Heurística CKAN:\n",
    "    - groups => vocabulario controlado oficial\n",
    "    - tags normalizados => vocabulario débil pero válido\n",
    "    \"\"\"\n",
    "    if groups:\n",
    "        return True, \"group\"\n",
    "\n",
    "    if tags:\n",
    "        # tags cortos, sin espacios largos, repetidos\n",
    "        normalized_tags = [t.get(\"name\",\"\") for t in tags if isinstance(t, dict)]\n",
    "        if any(len(t) <= 25 and \"-\" in t for t in normalized_tags):\n",
    "            return True, \"tag\"\n",
    "\n",
    "    return False, \"none\"\n",
    "\n",
    "def portal_supports_dcat(site_base: str) -> bool:\n",
    "    # Aragón: suele exponer catálogo DCAT (si responde, marcamos 1)\n",
    "    return head_or_get_public(urljoin(site_base.rstrip(\"/\") + \"/\", \"catalog/dcat.json\")) or \\\n",
    "           head_or_get_public(urljoin(site_base.rstrip(\"/\") + \"/\", \"catalog/dcat.rdf\"))\n",
    "\n",
    "def is_open_license(license_value: str) -> int:\n",
    "    s = (license_value or \"\").strip().lower()\n",
    "    if \"creative commons\" in s:\n",
    "        return 1\n",
    "    if \"cc-by\" in s:\n",
    "        return 1\n",
    "    if re.search(r\"\\bcc\\s*by\\b\", s):\n",
    "        return 1\n",
    "    if \"avisolegal\" in s:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def extract_identifier(ds: dict, exd: dict) -> str:\n",
    "    for k in [\"identifier\", \"dct:identifier\", \"dcat:identifier\"]:\n",
    "        v = ds.get(k) or exd.get(k)\n",
    "        if v:\n",
    "            return str(v)\n",
    "    # fallback fuerte\n",
    "    return str(ds.get(\"id\") or ds.get(\"name\") or \"\")\n",
    "\n",
    "def extract_doi(ds: dict, exd: dict) -> str:\n",
    "    doi_regex = r\"(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\"\n",
    "    for k in [\"doi\", \"DOI\"]:\n",
    "        v = ds.get(k) or exd.get(k)\n",
    "        if v:\n",
    "            m = re.search(doi_regex, str(v), flags=re.I)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    for v in exd.values():\n",
    "        if not v:\n",
    "            continue\n",
    "        m = re.search(doi_regex, str(v), flags=re.I)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def find_update_frequency(exd: dict) -> str:\n",
    "    for k in [\n",
    "        \"accrualPeriodicity\", \"dct:accrualPeriodicity\", \"dcat:accrualPeriodicity\",\n",
    "        \"accrual_periodicity\", \"accrualperiodicity\",\n",
    "        \"update_frequency\", \"updateFrequency\", \"update_frecuency\",\n",
    "        \"frequency\", \"frecuencia_actualizacion\", \"periodicity\", \"periodicidad\"\n",
    "    ]:\n",
    "        v = exd.get(k)\n",
    "        if v:\n",
    "            return str(v)\n",
    "    return \"No definido\"\n",
    "\n",
    "def find_update_frequency_from_rdf(dataset_name: str) -> str:\n",
    "    \"\"\"Intenta leer accrualPeriodicity desde el RDF del dataset.\"\"\"\n",
    "    if not dataset_name:\n",
    "        return \"No definido\"\n",
    "\n",
    "    rdf_url = f\"{SITE_BASE}/dataset/{dataset_name}.rdf\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(rdf_url, headers=UA_HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "        if r.status_code >= 400:\n",
    "            return \"No definido\"\n",
    "        txt = r.text\n",
    "\n",
    "        # Caso 1: recurso URI\n",
    "        m = re.search(r\"accrualPeriodicity[^>]+rdf:resource\\s*=\\s*\\\"([^\\\"]+)\\\"\", txt, flags=re.I)\n",
    "        if m:\n",
    "            uri = m.group(1).strip()\n",
    "            tail = uri.rstrip(\"/\").split(\"/\")[-1]\n",
    "            return tail\n",
    "\n",
    "        # Caso 2: literal\n",
    "        m2 = re.search(r\"accrualPeriodicity[^>]*>\\s*([^<]+)\\s*<\", txt, flags=re.I)\n",
    "        if m2:\n",
    "            return m2.group(1).strip()\n",
    "\n",
    "        return \"No definido\"\n",
    "    except Exception:\n",
    "        return \"No definido\"\n",
    "\n",
    "\n",
    "def find_update_frequency_from_catalog_html(dataset_uri: str) -> str:\n",
    "    \"\"\"Intenta extraer 'Frecuencia de actualización' desde el HTML del dataset.\"\"\"\n",
    "    if not dataset_uri or not str(dataset_uri).startswith(\"http\"):\n",
    "        return \"No definido\"\n",
    "    try:\n",
    "        r = requests.get(dataset_uri, headers=UA_HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "        if r.status_code >= 400:\n",
    "            return \"No definido\"\n",
    "        html = r.text\n",
    "\n",
    "        m = re.search(\n",
    "            r\"Frecuencia\\s+de\\s+actualizaci[oó]n.*?</[^>]+>\\s*([^<\\n\\r]+)\",\n",
    "            html,\n",
    "            flags=re.I | re.S\n",
    "        )\n",
    "        if m:\n",
    "            val = re.sub(r\"\\s{2,}\", \" \", m.group(1).strip())\n",
    "            return val[:80].strip()\n",
    "\n",
    "        return \"No definido\"\n",
    "    except Exception:\n",
    "        return \"No definido\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# CKAN fetch\n",
    "# -------------------------------\n",
    "def ckan_package_search(start=0, rows=ROWS):\n",
    "    url = urljoin(API_BASE, \"package_search\")\n",
    "    params = {\"start\": start, \"rows\": rows}\n",
    "    js = safe_get(url, params=params).json()\n",
    "    if not js.get(\"success\"):\n",
    "        raise RuntimeError(f\"CKAN package_search no success: {js}\")\n",
    "    return js[\"result\"]\n",
    "\n",
    "def ckan_package_show(ds_id):\n",
    "    url = urljoin(API_BASE, \"package_show\")\n",
    "    js = safe_get(url, params={\"id\": ds_id}).json()\n",
    "    if not js.get(\"success\"):\n",
    "        raise RuntimeError(f\"CKAN package_show no success: {ds_id}\")\n",
    "    return js[\"result\"]\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluación dataset -> fila\n",
    "# -------------------------------\n",
    "def evaluate_dataset_ckan(ds: dict, portal_has_dcat: bool):\n",
    "    extras = extras_to_dict(ds.get(\"extras\", []))\n",
    "\n",
    "    ds_id = ds.get(\"id\", \"\") or \"\"\n",
    "    name = ds.get(\"name\", \"\") or \"\"\n",
    "\n",
    "    title = ds.get(\"title\", \"\") or \"\"\n",
    "    notes = ds.get(\"notes\", \"\") or \"\"\n",
    "\n",
    "    groups = ds.get(\"groups\") or []\n",
    "    tags = ds.get(\"tags\") or []\n",
    "\n",
    "    category, category_source = classify_category_ckan(title=title, notes=notes, groups=groups, tags=tags)\n",
    "    uses_controlled_vocab, controlled_vocab_source = detect_controlled_vocab_ckan(groups=groups, tags=tags)\n",
    "\n",
    "    # Fechas: Aragón muchas veces no trae issued/modified \"puros\"; usamos los de CKAN cuando falten\n",
    "    # Fechas (raw)\n",
    "    issued_raw = extras.get(\"issued\") or extras.get(\"dct:issued\") or ds.get(\"issued\") or ds.get(\"metadata_created\") or \"\"\n",
    "    modified_raw = extras.get(\"modified\") or extras.get(\"dct:modified\") or ds.get(\"modified\") or ds.get(\"metadata_modified\") or \"\"\n",
    "\n",
    "    # Fechas normalizadas (UTC ISO)\n",
    "    issued_dt_utc = parse_dt(issued_raw)\n",
    "    modified_dt_utc = parse_dt(modified_raw)\n",
    "\n",
    "    issued_norm = issued_dt_utc.isoformat().replace(\"+00:00\", \"Z\") if issued_dt_utc else \"\"\n",
    "    modified_norm = modified_dt_utc.isoformat().replace(\"+00:00\", \"Z\") if modified_dt_utc else \"\"\n",
    "\n",
    "    meets_age_flag = 1 if meets_age(issued_dt_utc) else 0\n",
    "\n",
    "\n",
    "    # Publisher\n",
    "    publisher = \"\"\n",
    "    if isinstance(ds.get(\"organization\"), dict):\n",
    "        publisher = ds[\"organization\"].get(\"title\") or ds[\"organization\"].get(\"name\") or \"\"\n",
    "    publisher = publisher or ds.get(\"author\", \"\") or \"\"\n",
    "\n",
    "    # Licencia\n",
    "    license_text = ds.get(\"license_title\") or ds.get(\"license_id\") or \"\"\n",
    "    license_present = 1 if str(license_text).strip() else 0\n",
    "    license_open = is_open_license(license_text)\n",
    "\n",
    "    # Identificadores\n",
    "    identifier = extract_identifier(ds, extras)\n",
    "    doi = extract_doi(ds, extras)\n",
    "    has_doi = 1 if doi else 0\n",
    "\n",
    "    # Recursos\n",
    "    resources = ds.get(\"resources\") or []\n",
    "    formats = []\n",
    "    download_urls = []\n",
    "    doc_hint_urls = []\n",
    "\n",
    "    for r in resources:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        fmt = (r.get(\"format\") or \"\").strip()\n",
    "        if fmt:\n",
    "            formats.append(fmt.upper())\n",
    "\n",
    "        u = (r.get(\"url\") or \"\").strip()\n",
    "        if u:\n",
    "            download_urls.append(u)\n",
    "            doc_hint_urls.append(u)\n",
    "\n",
    "        desc_r = (r.get(\"description\") or \"\") + \" \" + (r.get(\"name\") or \"\")\n",
    "        if desc_r.strip():\n",
    "            doc_hint_urls.append(desc_r)\n",
    "\n",
    "    formats_unique = sorted(set([f for f in formats if f]))\n",
    "    format_join = \", \".join(formats_unique)\n",
    "    n_formats = len(formats_unique)\n",
    "\n",
    "    # metadata_rdf_available: comprobamos si existe el RDF del dataset\n",
    "    rdf_meta_url = RDF_TEMPLATE.format(name=name) if name else \"\"\n",
    "    metadata_rdf_available = 1 if (rdf_meta_url and head_or_get_public(rdf_meta_url)) else 0\n",
    "    if metadata_rdf_available and \"RDF\" not in formats_unique:\n",
    "        formats_unique.append(\"RDF\")\n",
    "        formats_unique = sorted(set(formats_unique))\n",
    "        format_join = \", \".join(formats_unique)\n",
    "        n_formats = len(formats_unique)\n",
    "\n",
    "    has_allowed_format = 1 if any(f in OPEN_FORMATS for f in formats_unique) else 0\n",
    "    has_semantic_serialization = 1 if any(f in SEMANTIC_FORMATS for f in formats_unique) else 0\n",
    "\n",
    "    # Data dictionary\n",
    "    has_data_dictionary = 1 if (\n",
    "        contains_any(title + \" \" + notes, DATA_DICT_PATTERNS) or\n",
    "        contains_any(\" \".join(doc_hint_urls), DATA_DICT_PATTERNS) or\n",
    "        any(k.lower() in (str(extras.keys()).lower()) for k in [\"schema\", \"dictionary\", \"diccionario\"])\n",
    "    ) else 0\n",
    "\n",
    "    # dataset_uri (EL QUE TÚ QUIERES)\n",
    "    dataset_uri = DATASET_LANDING_TEMPLATE.format(name=name) if name else (ds.get(\"url\") or \"\")\n",
    "\n",
    "\n",
    "    # download_url(s)\n",
    "    download_url = download_urls[0] if download_urls else \"\"\n",
    "\n",
    "    # update_frequency: extras -> rdf -> html\n",
    "    update_frequency = find_update_frequency(extras)\n",
    "    if update_frequency == \"No definido\":\n",
    "        update_frequency = find_update_frequency_from_rdf(name)  # usa el slug\n",
    "    if update_frequency == \"No definido\":\n",
    "        update_frequency = find_update_frequency_from_catalog_html(dataset_uri)  # usa URL real\n",
    "\n",
    "\n",
    "    frequency_documented = 1 if (update_frequency.strip() and update_frequency != \"No definido\") else 0\n",
    "\n",
    "    # public_access_ok\n",
    "    public_access_ok = 1 if (any(head_or_get_public(u) for u in download_urls) if download_urls else False) else 0\n",
    "\n",
    "    \n",
    "   # SE PUEDEN AGREGAR O QUITAR COLUMNAS DE RESULTADOS SIEMPRE QUE LAS DEFINAS \n",
    "    \n",
    "    return {\n",
    "        # ===== columnas que reclamaste (NO TOCAR) =====\n",
    "        \"portal\": PORTAL,\n",
    "        \"api_type\": API_TYPE,\n",
    "        \"portal_has_api_rest\": 1,\n",
    "        \"portal_supports_dcat_dcatap\": 1 if portal_has_dcat else 0,\n",
    "\n",
    "        \"identifier\": identifier,\n",
    "        \"doi\": doi,\n",
    "        \"has_doi\": has_doi,\n",
    "\n",
    "        \"publisher\": publisher,\n",
    "\n",
    "        \"download_url\": download_url,\n",
    "        \"download_urls\": \" | \".join(download_urls),\n",
    "\n",
    "        \"license\": license_text,\n",
    "        \"license_present\": license_present,\n",
    "        \"license_open\": license_open,\n",
    "\n",
    "        # ===== core de tu tabla punto 1 =====\n",
    "        \"dataset_id\": ds_id,\n",
    "        \"dataset_uri\": dataset_uri,\n",
    "        \"title\": title,\n",
    "        \"description\": notes,\n",
    "        \"category\": category,\n",
    "        \"category_source\": category_source,\n",
    "\n",
    "        \"uses_controlled_vocab\": 1 if uses_controlled_vocab else 0,\n",
    "        \"controlled_vocab_source\": controlled_vocab_source,\n",
    "\n",
    "        \"issued\": issued_raw,\n",
    "        \"modified\": modified_raw,\n",
    "        \"meets_age_criterion\": meets_age_flag,\n",
    "\n",
    "        \"format\": format_join,\n",
    "        \"n_formats\": n_formats,\n",
    "        \"metadata_rdf_available\": metadata_rdf_available,\n",
    "\n",
    "        \"has_allowed_format\": has_allowed_format,\n",
    "        \"has_semantic_serialization\": has_semantic_serialization,\n",
    "        \"has_data_dictionary\": has_data_dictionary,\n",
    "\n",
    "        \"update_frequency\": update_frequency,\n",
    "        \"frequency_documented\": frequency_documented,\n",
    "\n",
    "        \"public_access_ok\": public_access_ok,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    portal_has_dcat = portal_supports_dcat(SITE_BASE)\n",
    "\n",
    "    rows = []\n",
    "    start = 0\n",
    "    total = None\n",
    "\n",
    "    while True:\n",
    "        result = ckan_package_search(start=start, rows=ROWS)\n",
    "        if total is None:\n",
    "            total = int(result.get(\"count\", 0))\n",
    "\n",
    "        datasets = result.get(\"results\", []) or []\n",
    "        if not datasets:\n",
    "            break\n",
    "\n",
    "        for ds in datasets:\n",
    "            ds_full = ckan_package_show(ds.get(\"id\") or ds.get(\"name\"))\n",
    "            rows.append(evaluate_dataset_ckan(ds_full, portal_has_dcat))\n",
    "            time.sleep(SLEEP)\n",
    "\n",
    "            if MAX_DATASETS and len(rows) >= MAX_DATASETS:\n",
    "                break\n",
    "\n",
    "        if MAX_DATASETS and len(rows) >= MAX_DATASETS:\n",
    "            break\n",
    "\n",
    "        start += ROWS\n",
    "        if start >= total:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    \n",
    "  # SE PUEDEN CAMBIAR LOS NOMBRES DE LOS ARCHIVOS RESULTADOS CSV Y XLSX  \n",
    "    \n",
    "    \n",
    "    out_csv = \"Punto1_NOMBREPORTAL.csv\"\n",
    "    out_xlsx = \"Punto1_NOMBREPORTAL.xlsx\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    df.to_excel(out_xlsx, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"\\n✅ Listo: {PORTAL}\")\n",
    "    print(f\"Datasets extraídos: {len(df)} (MAX_DATASETS={MAX_DATASETS})\")\n",
    "    print(f\"Archivos: {out_csv} | {out_xlsx}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795addb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
