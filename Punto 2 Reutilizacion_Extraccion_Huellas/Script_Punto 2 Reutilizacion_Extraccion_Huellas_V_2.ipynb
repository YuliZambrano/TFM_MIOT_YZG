{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote, urlparse\n",
    "from collections import Counter\n",
    "from deep_translator import GoogleTranslator \n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# UTILIDADES GENERALES\n",
    "# ===============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9√°√©√≠√≥√∫√± ]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def translate_to_en(text):\n",
    "    \"\"\"Traducci√≥n de texto con manejo de errores.\"\"\"\n",
    "    try:\n",
    "        # Solo traduce si la longitud es razonable para evitar abusos o errores\n",
    "        if len(text.split()) < 3 and len(text) < 15:\n",
    "            return text\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "def extract_keywords(title, description, k=5):\n",
    "    \"\"\"Extrae palabras clave largas y √∫nicas.\"\"\"\n",
    "    text = clean_text((title or \"\") + \" \" + (description or \"\"))\n",
    "    stop = {\"de\", \"del\", \"la\", \"el\", \"y\", \"en\", \"por\", \"con\", \"los\", \"las\", \"un\", \"una\", \"o\", \"a\", \"para\", \"es\", \"que\", \"se\", \"como\", \"mas\"}\n",
    "    words = [w for w in text.split() if len(w) > 4 and w not in stop]\n",
    "    \n",
    "    # Usa Counter para obtener las palabras m√°s frecuentes\n",
    "    word_counts = Counter(words)\n",
    "    # Selecciona las palabras m√°s comunes hasta el l√≠mite k\n",
    "    return [word for word, count in word_counts.most_common(k)]\n",
    "\n",
    "\n",
    "def get_quoted_query(query, quoted):\n",
    "    \"\"\"Genera la cadena de consulta con o sin comillas.\"\"\"\n",
    "    query_clean = query.replace('\"', '').strip()\n",
    "    if not query_clean: return None\n",
    "    if quoted:\n",
    "        # B√∫squeda exacta (frase)\n",
    "        return quote(f'\"{query_clean}\"')\n",
    "    else:\n",
    "        # B√∫squeda amplia (palabras separadas)\n",
    "        return quote(query_clean)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# FUENTES DE REUTILIZACI√ìN (B√∫squeda por Contenido)\n",
    "# ===============================================================\n",
    "\n",
    "# ---------- CROSSREF ----------\n",
    "def search_crossref(query, quoted=True):\n",
    "    quoted_query = get_quoted_query(query, quoted)\n",
    "    if not quoted_query: return []\n",
    "    url = f\"https://api.crossref.org/works?query.full={quoted_query}&rows=10\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15).json()\n",
    "        items = r.get(\"message\", {}).get(\"items\", [])\n",
    "        results = []\n",
    "        for it in items:\n",
    "            results.append({\n",
    "                \"source\": \"crossref\",\n",
    "                \"external_id\": it.get(\"DOI\"),\n",
    "                \"title\": \" | \".join(it.get(\"title\", [])),\n",
    "                \"url\": it.get(\"URL\"),\n",
    "                \"date\": it.get(\"created\", {}).get(\"date-time\", \"\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------- OPENALEX ----------\n",
    "def search_openalex(query, quoted=True):\n",
    "    quoted_query = get_quoted_query(query, quoted)\n",
    "    if not quoted_query: return []\n",
    "    # default.search busca en t√≠tulo y abstract.\n",
    "    url = f\"https://api.openalex.org/works?filter=default.search:{quoted_query}&per-page=10\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15).json()\n",
    "        results = []\n",
    "        for it in r.get(\"results\", []):\n",
    "            results.append({\n",
    "                \"source\": \"openalex\",\n",
    "                \"external_id\": it.get(\"id\"),\n",
    "                \"title\": it.get(\"title\"),\n",
    "                \"url\": it.get(\"doi\"),\n",
    "                \"date\": it.get(\"publication_date\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------- ZENODO ----------\n",
    "def search_zenodo(query, quoted=True):\n",
    "    quoted_query = get_quoted_query(query, quoted)\n",
    "    if not quoted_query: return []\n",
    "    url = f\"https://zenodo.org/api/records/?q={quoted_query}&size=10\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15).json()\n",
    "        hits = r.get(\"hits\", {}).get(\"hits\", [])\n",
    "        results = []\n",
    "        for it in hits:\n",
    "            md = it.get(\"metadata\", {})\n",
    "            results.append({\n",
    "                \"source\": \"zenodo\",\n",
    "                \"external_id\": it.get(\"id\"),\n",
    "                \"title\": md.get(\"title\"),\n",
    "                \"url\": md.get(\"doi\"),\n",
    "                \"date\": md.get(\"publication_date\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------- GITHUB - SIN TOKEN ----------\n",
    "def search_github(query, is_uri=False):\n",
    "    # GitHub no necesita el par√°metro `quoted` en el mismo sentido, se maneja en q_final\n",
    "    \n",
    "    headers = {\"Accept\": \"application/vnd.github.v3.text-match+json\", \n",
    "               'User-Agent': 'TFM-Data-Reuse-Analyzer/1.0'}\n",
    "\n",
    "    if is_uri:\n",
    "        q_final = f'\"{query}\" in:file extension:md,ipynb,json,py'\n",
    "    elif \".\" in query:\n",
    "        q_final = f'filename:\"{query}\" in:path'\n",
    "    else:\n",
    "        # B√∫squeda por palabras clave sin comillas para ser amplio\n",
    "        q_final = query \n",
    "        if len(q_final.split()) > 1:\n",
    "            q_final = f'\"{q_final}\" in:readme' # Si es una frase, la busca en readme\n",
    "        else:\n",
    "            q_final = f'{q_final} in:readme'\n",
    "        \n",
    "    url = f\"https://api.github.com/search/code?q={quote(q_final)}&per_page=10\"\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=15).json()\n",
    "        items = r.get(\"items\", [])\n",
    "        \n",
    "        # üö® Manejo de Rate Limit (CR√çTICO sin Token)\n",
    "        if 'message' in r and 'rate limit' in r['message']:\n",
    "            print(\"üö® L√≠mite de tasa de GitHub alcanzado (sin token). Pausando 60s...\")\n",
    "            time.sleep(60) \n",
    "            return []\n",
    "        if 'message' in r and r['message'] == 'Not Found': \n",
    "            return []\n",
    "            \n",
    "        results = []\n",
    "        for it in items:\n",
    "            repo = it.get(\"repository\", {})\n",
    "            results.append({\n",
    "                \"source\": \"github\",\n",
    "                \"external_id\": repo.get(\"full_name\"),\n",
    "                \"title\": it.get(\"name\"),\n",
    "                \"url\": it.get(\"html_url\"),\n",
    "                \"date\": repo.get(\"pushed_at\") or repo.get(\"created_at\", \"\")\n",
    "            })\n",
    "        return results\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------- KAGGLE (Scraping de datasets) ----------\n",
    "def search_kaggle(query, quoted=True):\n",
    "    # Kaggle API no usa el concepto de quoted/unquoted, la URL de b√∫squeda es simple\n",
    "    query_clean = get_quoted_query(query, False) # Solo quotea sin comillas\n",
    "    if not query_clean: return []\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://www.kaggle.com/datasets?search={query_clean}\"\n",
    "        r = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = r.text\n",
    "        titles = re.findall(r'data-testid=\"title\">(.*?)</span>', html)\n",
    "        links  = re.findall(r'href=\"(/datasets/[^\"]+)\"', html)\n",
    "\n",
    "        results = []\n",
    "        for i, t in enumerate(titles[:5]):\n",
    "            link = links[i] if i < len(links) else \"\"\n",
    "            results.append({\n",
    "                \"source\": \"kaggle\",\n",
    "                \"external_id\": \"\",\n",
    "                \"title\": clean_text(t),\n",
    "                \"url\": \"https://www.kaggle.com\" + link if link else \"\",\n",
    "                \"date\": \"\"\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------- CORDIS ----------\n",
    "def search_cordis(query, quoted=True):\n",
    "    quoted_query = get_quoted_query(query, quoted)\n",
    "    if not quoted_query: return []\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://cordis.europa.eu/api/search?q={quoted_query}&num=5\"\n",
    "        r = requests.get(url, timeout=15).json()\n",
    "        results = []\n",
    "        for it in r.get(\"projects\", []):\n",
    "            results.append({\n",
    "                \"source\": \"cordis\",\n",
    "                \"external_id\": it.get(\"id\"),\n",
    "                \"title\": it.get(\"title\"),\n",
    "                \"url\": f\"https://cordis.europa.eu/project/id/{it.get('id')}\",\n",
    "                \"date\": it.get(\"startDate\", \"\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# FUNCI√ìN PRINCIPAL DE B√öSQUEDA JER√ÅRQUICA (REFUERZO DE KEYWORDS)\n",
    "# ===============================================================\n",
    "\n",
    "def search_reuse(r):\n",
    "\n",
    "    dataset_uri = r[\"dataset_uri\"]\n",
    "    title = r[\"title\"]\n",
    "    #  CR√çTICO: Necesitamos la descripci√≥n para extraer palabras clave.\n",
    "    description = r[\"description\"] \n",
    "    file_names = r[\"file_names\"]\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    # 1. ALTA PRECISI√ìN: URI Exacta\n",
    "    queries.append({\"q\": dataset_uri, \"match_type\": \"URI_STRICT\", \"is_uri\": True, \"quoted\": True})\n",
    "\n",
    "    # 2. MEDIA PRECISI√ìN: Nombres de archivo √∫nicos\n",
    "    if isinstance(file_names, str):\n",
    "        file_names = eval(file_names) # Convierte la cadena de lista a lista real si viene de Excel\n",
    "    \n",
    "    for fn in file_names:\n",
    "        if len(fn) > 8 and '.' in fn:\n",
    "            queries.append({\"q\": fn, \"match_type\": \"FILENAME\", \"is_uri\": False, \"quoted\": True})\n",
    "\n",
    "    # 3. BAJA PRECISI√ìN: T√≠tulo Exacto (como frase)\n",
    "    title_clean = clean_text(title)\n",
    "    if len(title_clean.split()) > 3:\n",
    "        queries.append({\"q\": title_clean, \"match_type\": \"TITLE_EXACT\", \"is_uri\": False, \"quoted\": True})\n",
    "\n",
    "    # 4.  NUEVO: BAJA PRECISI√ìN (AMPLIA): Palabras clave traducidas (crucial para OpenAlex/Crossref)\n",
    "    keywords_es = extract_keywords(title, description, k=5)\n",
    "    \n",
    "    if keywords_es:\n",
    "        # Busca las palabras clave combinadas, traducidas y SIN comillas para b√∫squeda amplia\n",
    "        keywords_en = \" \".join([translate_to_en(k) for k in keywords_es])\n",
    "        \n",
    "        # Debe haber al menos dos palabras clave para una b√∫squeda significativa\n",
    "        if keywords_en and len(keywords_en.split()) > 1:\n",
    "            queries.append({\"q\": keywords_en, \"match_type\": \"KEYWORD_BROAD\", \"is_uri\": False, \"quoted\": False})\n",
    "            \n",
    "    results = []\n",
    "    \n",
    "    sources = [\n",
    "        (search_openalex, \"openalex\"),\n",
    "        (search_crossref, \"crossref\"),\n",
    "        (search_zenodo, \"zenodo\"),\n",
    "        (search_github, \"github\"),\n",
    "        (search_kaggle, \"kaggle\"),\n",
    "        (search_cordis, \"cordis\"),\n",
    "    ]\n",
    "\n",
    "    for query_data in queries:\n",
    "        q = query_data[\"q\"]\n",
    "        match_type = query_data[\"match_type\"]\n",
    "        is_uri = query_data[\"is_uri\"]\n",
    "        quoted = query_data[\"quoted\"]\n",
    "        \n",
    "        if not q or not str(q).strip(): continue\n",
    "\n",
    "        for fn, src in sources:\n",
    "            \n",
    "            # GitHub tiene un manejo especial de la b√∫squeda\n",
    "            if src == \"github\":\n",
    "                hits = fn(q, is_uri=is_uri)\n",
    "            else:\n",
    "                # El resto de fuentes usan el flag 'quoted'\n",
    "                hits = fn(q, quoted=quoted)\n",
    "\n",
    "            for h in hits:\n",
    "                h[\"match_type\"] = match_type\n",
    "                results.append(h)\n",
    "\n",
    "        time.sleep(0.5 + random.random() * 0.5)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MAIN PUNTO 2\n",
    "# ===============================================================\n",
    "\n",
    "def main():\n",
    "\n",
    "    #  NOTA: AQUI LOS ARCHIVOS\n",
    "    INPUT = \"Punto1_NOMBREARCHIVORESULTADO.xlsx\"                # ARCHIVO RESULTADO DEL SCRIPT 1\n",
    "    OUTPUT = \"Punto2_NOMBREARCHIVORESULTADO.xlsx\"   # ARCHIVO QUE RESULTADO DE ESTE SCRIPT 2\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(INPUT)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontr√≥ el archivo de entrada '{INPUT}'. Aseg√∫rate de ejecutar el Punto 1 y que el archivo exista.\")\n",
    "        return\n",
    "\n",
    "    #  CR√çTICO: Asegurarse de que las columnas necesarias existan\n",
    "    required_cols = [\"dataset_uri\", \"title\", \"description\", \"file_names\", \"category\", \"issued\"]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: El archivo de entrada '{INPUT}' no contiene todas las columnas requeridas para la b√∫squeda de reutilizaci√≥n ({', '.join(required_cols)}).\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    counter = Counter()\n",
    "\n",
    "    print(\"\\n Iniciando b√∫squeda OPTIMIZADA y REFORZADA de reutilizaci√≥n (V5)...\\n\")\n",
    "    print(f\"Total de datasets a procesar: {len(df)}\")\n",
    "\n",
    "    for idx, r in df.iterrows():\n",
    "\n",
    "        print(f\"[{idx+1}/{len(df)}] ‚Üí Buscando para: {r['title']}\")\n",
    "\n",
    "        hits = search_reuse(r)\n",
    "\n",
    "        for h in hits:\n",
    "            rows.append({\n",
    "                \"dataset_uri\": r[\"dataset_uri\"],\n",
    "                \"title_dataset\": r[\"title\"],\n",
    "                \"category\": r[\"category\"],\n",
    "                \"issued\": r[\"issued\"],\n",
    "                \n",
    "                \"source\": h.get(\"source\"),\n",
    "                \"external_id\": h.get(\"external_id\"),\n",
    "                \"match_type\": h.get(\"match_type\"),\n",
    "                \"source_title\": h.get(\"title\"),\n",
    "                \"url\": h.get(\"url\"),\n",
    "                \"date\": h.get(\"date\")\n",
    "            })\n",
    "\n",
    "            counter[h.get(\"source\")] += 1\n",
    "        \n",
    "    out = pd.DataFrame(rows)\n",
    "    \n",
    "    # Desambiguaci√≥n: Prioriza la URI_STRICT y elimina duplicados de huella\n",
    "    match_order = {'URI_STRICT': 0, 'FILENAME': 1, 'TITLE_EXACT': 2, 'KEYWORD_BROAD': 3}\n",
    "    out_final = out.sort_values(by='match_type', key=lambda x: x.map(match_order), ascending=True)\n",
    "    out_final = out_final.drop_duplicates(subset=['dataset_uri', 'external_id', 'source'], keep='first')\n",
    "    \n",
    "    out_final.to_excel(OUTPUT, index=False)\n",
    "\n",
    "    print(\"\\n Archivo generado correctamente ‚Üí\", OUTPUT)\n",
    "\n",
    "    print(\"\\n Recuento de huellas √∫nicas por fuente:\\n\")\n",
    "    for src, cnt in out_final[\"source\"].value_counts().items():\n",
    "        print(f\"  {src}: {cnt} huellas √∫nicas\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
