{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote, urlparse\n",
    "from collections import Counter\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ==========================================================\n",
    "\n",
    "INPUT_FILE  = \"Punto1_NOMBREARCHIVORESULTADO.xlsx\"   # ARCHIVO RESULTADO DEL SCRIPT 1\n",
    "OUTPUT_FILE = \"Punto2_NOMBREARCHIVORESULTADO.xlsx\"   # ARCHIVO QUE RESULTADO DE ESTE SCRIPT 2\n",
    "PORTAL_NAME = \"NOMBRE DEL PORTAL\"                   # NOMBRE DEL PORTAL\n",
    "\n",
    "# AQUI PUEDES USAR TU TOKEN DE GitHub, ponlo aqu√≠ para mejores resultados\n",
    "GITHUB_TOKEN = None  # \"EJEMPLO_xxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# UTILIDADES\n",
    "# ==========================================================\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_dataset_id(dataset_uri: str) -> str:\n",
    "    try:\n",
    "        return dataset_uri.rstrip(\"/\").split(\"/\")[-1]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_domain(dataset_uri: str) -> str:\n",
    "    try:\n",
    "        return urlparse(dataset_uri).netloc\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_file_tokens(download_url: str):\n",
    "    \"\"\"\n",
    "    A partir de la columna download_url genera tokens fuertes:\n",
    "    nombres de ficheros sin extensi√≥n (transporte-bus, paradas-autobus, Ferris, TODO ESTO EJEMPLOSetc.).\n",
    "    \"\"\"\n",
    "    tokens = set()\n",
    "    if not isinstance(download_url, str):\n",
    "        return tokens\n",
    "\n",
    "    parts = [p.strip() for p in download_url.split(\";\") if p.strip()]\n",
    "    for p in parts:\n",
    "        # nombre de archivo\n",
    "        fname = p.split(\"/\")[-1]\n",
    "        if not fname:\n",
    "            continue\n",
    "        base = fname.split(\".\")[0]  # sin extensi√≥n\n",
    "        base_clean = clean_text(base)\n",
    "        if base_clean:\n",
    "            tokens.add(base_clean)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_strong_tokens(dataset_uri: str, title: str, download_url: str):\n",
    "    \"\"\"\n",
    "    Construye un conjunto de tokens \"fuertes\" para detectar reutilizaci√≥n real:\n",
    "    - slug del dataset (√∫ltimo segmento de la URI)\n",
    "    - nombres de ficheros de descarga (sin extensi√≥n)\n",
    "    - t√≠tulo del dataset normalizado completo\n",
    "    \"\"\"\n",
    "    tokens = set()\n",
    "\n",
    "    # slug\n",
    "    slug = extract_dataset_id(dataset_uri)\n",
    "    slug_clean = clean_text(slug)\n",
    "    if slug_clean:\n",
    "        tokens.add(slug_clean)\n",
    "\n",
    "    # t√≠tulo completo normalizado\n",
    "    title_clean = clean_text(title)\n",
    "    if title_clean:\n",
    "        tokens.add(title_clean)\n",
    "\n",
    "    # tokens de archivos\n",
    "    tokens |= extract_file_tokens(download_url)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# FUENTES DE REUTILIZACI√ìN\n",
    "# (todas se consultan con identificadores fuertes)\n",
    "# ==========================================================\n",
    "\n",
    "def search_crossref(query: str):\n",
    "    url = f\"https://api.crossref.org/works?query={quote(query)}&rows=5\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10).json()\n",
    "        items = r.get(\"message\", {}).get(\"items\", [])\n",
    "        results = []\n",
    "        for it in items:\n",
    "            results.append({\n",
    "                \"source\": \"crossref\",\n",
    "                \"external_id\": it.get(\"DOI\"),\n",
    "                \"title\": \" | \".join(it.get(\"title\", [])),\n",
    "                \"url\": it.get(\"URL\"),\n",
    "                \"date\": it.get(\"created\", {}).get(\"date-time\", \"\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_openalex(query: str):\n",
    "    # Lo mantenemos por completitud, pero ser√° filtrado igual que crossref\n",
    "    url = f\"https://api.openalex.org/works?filter=title.search:{quote(query)}&per-page=5\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10).json()\n",
    "        results = []\n",
    "        for it in r.get(\"results\", []):\n",
    "            results.append({\n",
    "                \"source\": \"openalex\",\n",
    "                \"external_id\": it.get(\"id\"),\n",
    "                \"title\": it.get(\"title\"),\n",
    "                \"url\": it.get(\"doi\"),\n",
    "                \"date\": it.get(\"publication_date\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_zenodo(query: str):\n",
    "    url = f\"https://zenodo.org/api/records/?q={quote(query)}&size=5\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10).json()\n",
    "        hits = r.get(\"hits\", {}).get(\"hits\", [])\n",
    "        results = []\n",
    "        for it in hits:\n",
    "            md = it.get(\"metadata\", {})\n",
    "            results.append({\n",
    "                \"source\": \"zenodo\",\n",
    "                \"external_id\": it.get(\"id\"),\n",
    "                \"title\": md.get(\"title\"),\n",
    "                \"url\": md.get(\"doi\"),\n",
    "                \"date\": md.get(\"publication_date\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_github(query: str):\n",
    "    headers = {\"Accept\": \"application/vnd.github+json\"}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "\n",
    "    q = query.strip()\n",
    "    if len(q) > 80:\n",
    "        q = q[:80]\n",
    "\n",
    "    url = f\"https://api.github.com/search/code?q={quote(q)}+in:file&per_page=5\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=10).json()\n",
    "        items = r.get(\"items\", [])\n",
    "        results = []\n",
    "        for it in items:\n",
    "            repo = it.get(\"repository\", {})\n",
    "            results.append({\n",
    "                \"source\": \"github\",\n",
    "                \"external_id\": repo.get(\"full_name\"),\n",
    "                \"title\": it.get(\"name\"),\n",
    "                \"url\": it.get(\"html_url\"),\n",
    "                \"date\": repo.get(\"pushed_at\") or repo.get(\"created_at\", \"\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_kaggle(query: str):\n",
    "    try:\n",
    "        url = f\"https://www.kaggle.com/datasets?search={quote(query)}\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        html = r.text\n",
    "\n",
    "        titles = re.findall(r'data-testid=\"title\">(.*?)</span>', html)\n",
    "        links  = re.findall(r'href=\"(/datasets/[^\"]+)\"', html)\n",
    "\n",
    "        results = []\n",
    "        for i, t in enumerate(titles[:5]):\n",
    "            url_rel = links[i] if i < len(links) else \"\"\n",
    "            results.append({\n",
    "                \"source\": \"kaggle\",\n",
    "                \"external_id\": \"\",\n",
    "                \"title\": t,\n",
    "                \"url\": \"https://www.kaggle.com\" + url_rel if url_rel else \"\",\n",
    "                \"date\": \"\"\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_cordis(query: str):\n",
    "    try:\n",
    "        url = f\"https://cordis.europa.eu/api/search?q={quote(query)}&num=5\"\n",
    "        r = requests.get(url, timeout=10).json()\n",
    "        projects = r.get(\"projects\") or r.get(\"results\") or []\n",
    "        results = []\n",
    "        for it in projects:\n",
    "            results.append({\n",
    "                \"source\": \"cordis\",\n",
    "                \"external_id\": it.get(\"id\"),\n",
    "                \"title\": it.get(\"title\", \"\"),\n",
    "                \"url\": str(it.get(\"rcn\", \"\")),\n",
    "                \"date\": it.get(\"startDate\", \"\")\n",
    "            })\n",
    "        return results\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_google_play(query: str):\n",
    "    try:\n",
    "        url = f\"https://play.google.com/store/search?q={quote(query)}&c=apps\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        html = r.text.lower()\n",
    "        apps = re.findall(r'\\\"title\\\":\\{\\\"label\\\":\\\"(.*?)\\\"', html)\n",
    "        return [{\n",
    "            \"source\": \"google_play\",\n",
    "            \"external_id\": \"\",\n",
    "            \"title\": a,\n",
    "            \"url\": \"\",\n",
    "            \"date\": \"\"\n",
    "        } for a in apps[:5]]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_appstore(query: str):\n",
    "    try:\n",
    "        url = f\"https://apps.apple.com/es/search?term={quote(query)}\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        html = r.text\n",
    "        apps = re.findall(\n",
    "            r'<h3 class=\"we-truncate we-truncate--single-line we-truncate--interactive we-truncate--multi-line-medium\">([^<]+)</h3>',\n",
    "            html\n",
    "        )\n",
    "        return [{\n",
    "            \"source\": \"app_store\",\n",
    "            \"external_id\": \"\",\n",
    "            \"title\": a,\n",
    "            \"url\": \"\",\n",
    "            \"date\": \"\"\n",
    "        } for a in apps[:5]]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# REUTILIZACI√ìN REAL (incluye Crossref/Zenodo)\n",
    "# ==========================================================\n",
    "\n",
    "def is_real_reuse(hit: dict,\n",
    "                  dataset_uri: str,\n",
    "                  strong_tokens: set,\n",
    "                  portal_markers: set,\n",
    "                  query_used: str) -> bool:\n",
    "    \"\"\"\n",
    "    Devuelve True SOLO si el resultado hace referencia real al dataset:\n",
    "    - contiene la URL del dataset, o\n",
    "    - contiene alg√∫n token fuerte (slug, nombre de fichero, t√≠tulo normalizado), o\n",
    "    - contiene alg√∫n marcador del portal (datosabiertos.regiondemurcia.es, datos.lorca.es, nexo.carm.es)\n",
    "    \"\"\"\n",
    "\n",
    "    source = hit.get(\"source\", \"\")\n",
    "    title  = clean_text(hit.get(\"title\", \"\") or \"\")\n",
    "    url    = clean_text(hit.get(\"url\", \"\") or \"\")\n",
    "    extid  = clean_text(str(hit.get(\"external_id\", \"\") or \"\"))\n",
    "    q      = clean_text(query_used or \"\")\n",
    "\n",
    "    full_text = \" \".join([title, url, extid, q])\n",
    "\n",
    "    ds_uri  = clean_text(dataset_uri or \"\")\n",
    "\n",
    "    # 1) URL EXACTA del dataset (muy fuerte)\n",
    "    if ds_uri and ds_uri in full_text:\n",
    "        return True\n",
    "\n",
    "    # 2) Marcadores de portal + slug / token fuerte\n",
    "    for mk in portal_markers:\n",
    "        mk_clean = clean_text(mk)\n",
    "        if mk_clean and mk_clean in full_text:\n",
    "            # si adem√°s aparece alg√∫n token fuerte, es reutilizaci√≥n clara\n",
    "            for tok in strong_tokens:\n",
    "                tok_clean = clean_text(tok)\n",
    "                if tok_clean and tok_clean in full_text:\n",
    "                    return True\n",
    "            # o, si el propio marcador es muy espec√≠fico (ej. datos.lorca.es/calle-cortada)\n",
    "            if \"/\" in mk and mk_clean in full_text:\n",
    "                return True\n",
    "\n",
    "    # 3) Tokens fuertes solos (slug, nombres de fichero, t√≠tulo exacto)\n",
    "    for tok in strong_tokens:\n",
    "        tok_clean = clean_text(tok)\n",
    "        if tok_clean and tok_clean in full_text:\n",
    "            return True\n",
    "\n",
    "    # Si no pasa ning√∫n criterio, NO consideramos reutilizaci√≥n\n",
    "    return False\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CONSTRUCCI√ìN DE QUERIES POR DATASET\n",
    "# ==========================================================\n",
    "\n",
    "def build_queries(dataset_uri: str,\n",
    "                  strong_tokens: set,\n",
    "                  portal_markers: set):\n",
    "    \"\"\"\n",
    "    Construye una lista corta de queries \"fuertes\":\n",
    "    - la URI completa del dataset\n",
    "    - el dominio\n",
    "    - algunos tokens fuertes (slug, nombres de fichero)\n",
    "    - marcadores de portal\n",
    "    \"\"\"\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    ds_uri = dataset_uri.strip()\n",
    "    if ds_uri:\n",
    "        queries.append((ds_uri, \"URI_STRICT\"))\n",
    "\n",
    "    domain = extract_domain(dataset_uri)\n",
    "    if domain:\n",
    "        queries.append((domain, \"DOMAIN_STRICT\"))\n",
    "\n",
    "    # tokens fuertes (slug + nombres fichero + t√≠tulo normalizado)\n",
    "    # limitamos a 3 para no disparar APIs\n",
    "    for tok in list(strong_tokens)[:3]:\n",
    "        if tok:\n",
    "            queries.append((tok, \"TOKEN_STRONG\"))\n",
    "\n",
    "    # marcadores del portal (datosabiertos.regiondemurcia.es, datos.lorca.es, nexo.carm.es)\n",
    "    for mk in portal_markers:\n",
    "        if mk:\n",
    "            queries.append((mk, \"PORTAL_MARKER\"))\n",
    "\n",
    "    # eliminar duplicados manteniendo orden\n",
    "    seen = set()\n",
    "    final_queries = []\n",
    "    for q, mt in queries:\n",
    "        q_norm = q.strip()\n",
    "        if not q_norm:\n",
    "            continue\n",
    "        key = (q_norm, mt)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            final_queries.append((q_norm, mt))\n",
    "\n",
    "    return final_queries[:8]\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# B√öSQUEDA MULTIFUENTE POR DATASET\n",
    "# ==========================================================\n",
    "\n",
    "def search_reuse(dataset_uri: str,\n",
    "                 title: str,\n",
    "                 download_url: str):\n",
    "    strong_tokens = build_strong_tokens(dataset_uri, title, download_url)\n",
    "\n",
    "    # marcadores globales del portal Murcia\n",
    "    portal_markers = {\n",
    "        \"https://datosabiertos.regiondemurcia.es\",\n",
    "        \"datosabiertos.regiondemurcia.es\",\n",
    "        \"http://datos.lorca.es\",\n",
    "        \"datos.lorca.es\",\n",
    "        \"http://nexo.carm.es\",\n",
    "        \"nexo.carm.es\",\n",
    "    }\n",
    "\n",
    "    queries = build_queries(dataset_uri, strong_tokens, portal_markers)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    sources = [\n",
    "        (search_crossref,    \"crossref\"),\n",
    "        (search_openalex,    \"openalex\"),\n",
    "        (search_zenodo,      \"zenodo\"),\n",
    "        (search_github,      \"github\"),\n",
    "        (search_kaggle,      \"kaggle\"),\n",
    "        (search_cordis,      \"cordis\"),\n",
    "        (search_google_play, \"google_play\"),\n",
    "        (search_appstore,    \"app_store\"),\n",
    "    ]\n",
    "\n",
    "    for q, match_type in queries:\n",
    "        for fn, src in sources:\n",
    "            hits = fn(q)\n",
    "            for h in hits:\n",
    "                if is_real_reuse(h, dataset_uri, strong_tokens, portal_markers, q):\n",
    "                    h[\"match_type\"] = match_type\n",
    "                    h[\"query_used\"] = q\n",
    "                    results.append(h)\n",
    "        # Pausa para no abusar de las APIs\n",
    "        time.sleep(0.25 + random.random() * 0.25)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "    rows = []\n",
    "    counter = Counter()\n",
    "\n",
    "    print(f\"\\nüîé Iniciando b√∫squeda de reutilizaci√≥n REAL (con Crossref/Zenodo filtrados) ‚Äì {PORTAL_NAME}\\n\")\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        dataset_uri  = r.get(\"dataset_uri\", \"\")\n",
    "        title        = r.get(\"title\", \"\")\n",
    "        download_url = r.get(\"download_url\", \"\")\n",
    "        issued       = r.get(\"issued\")\n",
    "\n",
    "        print(f\" ‚Üí Buscando reutilizaci√≥n para: {title}\")\n",
    "\n",
    "        hits = search_reuse(dataset_uri, title, download_url)\n",
    "#SE PUEDEN MODIFICAR LOS RETORNOS RESULTADOS SIEMPRE QUE LOS DEFINAS EN EL SCRIPT   \n",
    "        for h in hits:\n",
    "            rows.append({\n",
    "                \"dataset_uri\":   dataset_uri,\n",
    "                \"title_dataset\": title,\n",
    "                \"issued\":        issued,\n",
    "                \"source\":        h.get(\"source\"),\n",
    "                \"external_id\":   h.get(\"external_id\"),\n",
    "                \"match_type\":    h.get(\"match_type\"),\n",
    "                \"query_used\":    h.get(\"query_used\"),\n",
    "                \"source_title\":  h.get(\"title\"),\n",
    "                \"url\":           h.get(\"url\"),\n",
    "                \"date\":          h.get(\"date\"),\n",
    "            })\n",
    "            counter[h.get(\"source\")] += 1\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"\\n Archivo generado correctamente ‚Üí {OUTPUT_FILE}\\n\")\n",
    "    print(f\" Recuento de huellas de reutilizaci√≥n REAL (filtradas) ‚Äì {PORTAL_NAME}:\\n\")\n",
    "    if not counter:\n",
    "        print(\"  (No se han encontrado huellas estrictas de reutilizaci√≥n en ninguna fuente)\")\n",
    "    else:\n",
    "        for src, cnt in counter.items():\n",
    "            print(f\"  {src}: {cnt} coincidencias\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
