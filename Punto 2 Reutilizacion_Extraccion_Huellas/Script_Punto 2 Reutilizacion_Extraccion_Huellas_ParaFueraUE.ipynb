{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a025a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Buscando reutilizaciÃ³n: Annual Population Survey, 2004-2024: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study, Sweeps 1-7, 2001-2019: Exact Participation Dates: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study, Sweeps 1-7, 2001-2019: Demographics, Language and Religion: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study, Sweeps 1-7, 2001-2019: Self-Reported Health, Behaviour and Fertility: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study, Sweeps 1-7, 2001-2019: Socio-Economic, Accommodation and Occupational Data: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 17, Sweep 7, 2018\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Sweeps 1-7, 2001-2018: Longitudinal Family File\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 14, Sweep 6, 2015\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 11, Sweep 5, 2012\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 9 months, Sweep 1, 2001-2003: Health Visitor Survey\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 7, Sweep 4, 2008: Physical Activity\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 7, Sweep 4, 2008\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 5, Sweep 3, 2006\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 9 months, Sweep 1, 2003: Survey of Mothers who Received Assisted Fertility Treatment\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 3, Sweep 2, 2004\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Age 9 months, Sweep 1, 2001\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: National Travel Survey, 2002-2024: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: National Travel Survey, 2002-2024: Special Licence Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: National Travel Survey, 2002-2024\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: National Travel Survey, 1995-2001: Special Licence Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2025: Open Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2025\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2025: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Quarterly Labour Force Survey, July - September, 2025\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: 1970 British Cohort Study: Age 16, Sweep 4 Leisure and Television Diaries, 1986\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: 1970 British Cohort Study: Age 46, Sweep 10, 2016-2018\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: National Child Development Study: Local Authority Data, 1958-1974: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Annual Population Survey Household, 2004-2022: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Quarterly Labour Force Survey, 1992-2024: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Annual Population Survey, 2004-2023: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Millennium Cohort Study: Linked Health Administrative Datasets (Hospital Episode Statistics), England, 2000-2023: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Living Costs and Food Survey, 2022-2023\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024: Open Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024: Open Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Living Costs and Food Survey, 2022-2023\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Living Costs and Food Survey, 2022-2023\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Annual Business Survey, 2005-2022: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024: Secure Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024: Open Access\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: People and Nature Survey for England, 2020-2024\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Labour Force Survey Two-Quarter Longitudinal Dataset, April - September, 2024\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Understanding Society: Innovation Panel, Waves 1-16, 2008-2023: Special Licence Access, Westminster Parliamentary Constituencies\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Understanding Society: Innovation Panel, Waves 1-16, 2008-2023: Special Licence Access, Local Authority District\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Understanding Society: Innovation Panel, Waves 1-16, 2008-2023: Special Licence Access, Interviewer Characteristics\n",
      "ðŸ”Ž Buscando reutilizaciÃ³n: Understanding Society: Innovation Panel, Waves 1-16, 2008-2023: Secure Access, National Grid Reference (Easting, Northing, OSGRDIND)\n",
      "\n",
      "âœ” Archivo generado: Punto2_UK_ReutilizacionV1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# =========================================================\n",
    "# UTILIDADES\n",
    "# =========================================================\n",
    "\n",
    "def translate_to_en(text):\n",
    "    try:\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.lower().strip()\n",
    "    t = re.sub(r\"[^a-zA-Z0-9 ]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "\n",
    "def extract_keywords(title, description, k=6):\n",
    "    text = clean_text(f\"{title} {description}\")\n",
    "    stop = {\"the\", \"of\", \"and\", \"for\", \"data\", \"study\"}\n",
    "    words = [w for w in text.split() if len(w) > 4 and w not in stop]\n",
    "    return list(dict.fromkeys(words))[:k]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# FUENTES DE REUTILIZACIÃ“N\n",
    "# =========================================================\n",
    "\n",
    "def search_crossref(query):\n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works?query={quote(query)}&rows=5\"\n",
    "        items = requests.get(url, timeout=10).json().get(\"message\", {}).get(\"items\", [])\n",
    "        return [{\n",
    "            \"source\": \"crossref\",\n",
    "            \"external_id\": it.get(\"DOI\"),\n",
    "            \"title\": \" | \".join(it.get(\"title\", [])),\n",
    "            \"url\": it.get(\"URL\"),\n",
    "            \"date\": it.get(\"created\", {}).get(\"date-time\", \"\")\n",
    "        } for it in items]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_openalex(query):\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/works?filter=title.search:{quote(query)}&per-page=5\"\n",
    "        items = requests.get(url, timeout=10).json().get(\"results\", [])\n",
    "        return [{\n",
    "            \"source\": \"openalex\",\n",
    "            \"external_id\": it.get(\"id\"),\n",
    "            \"title\": it.get(\"title\"),\n",
    "            \"url\": it.get(\"doi\"),\n",
    "            \"date\": it.get(\"publication_date\")\n",
    "        } for it in items]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_zenodo(query):\n",
    "    try:\n",
    "        url = f\"https://zenodo.org/api/records/?q={quote(query)}&size=5\"\n",
    "        items = requests.get(url, timeout=10).json().get(\"hits\", {}).get(\"hits\", [])\n",
    "        return [{\n",
    "            \"source\": \"zenodo\",\n",
    "            \"external_id\": it.get(\"id\"),\n",
    "            \"title\": it.get(\"metadata\", {}).get(\"title\"),\n",
    "            \"url\": it.get(\"metadata\", {}).get(\"doi\"),\n",
    "            \"date\": it.get(\"metadata\", {}).get(\"publication_date\")\n",
    "        } for it in items]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_cordis(query):\n",
    "    try:\n",
    "        url = f\"https://cordis.europa.eu/api/search?q={quote(query)}&num=5\"\n",
    "        items = requests.get(url, timeout=10).json().get(\"projects\", [])\n",
    "        return [{\n",
    "            \"source\": \"cordis\",\n",
    "            \"external_id\": it.get(\"id\"),\n",
    "            \"title\": it.get(\"title\"),\n",
    "            \"url\": f\"https://cordis.europa.eu/project/id/{it.get('id')}\",\n",
    "            \"date\": it.get(\"startDate\")\n",
    "        } for it in items]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_github(query):\n",
    "    try:\n",
    "        url = f\"https://api.github.com/search/code?q={quote(query)}+in:file&per_page=5\"\n",
    "        res = requests.get(url, timeout=10).json().get(\"items\", [])\n",
    "        return [{\n",
    "            \"source\": \"github\",\n",
    "            \"external_id\": it.get(\"repository\", {}).get(\"full_name\"),\n",
    "            \"title\": it.get(\"name\"),\n",
    "            \"url\": it.get(\"html_url\"),\n",
    "            \"date\": it.get(\"repository\", {}).get(\"pushed_at\")\n",
    "        } for it in res]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BÃšSQUEDA PRINCIPAL\n",
    "# =========================================================\n",
    "\n",
    "def search_reuse(dataset_uri, title, description, doi):\n",
    "\n",
    "    title_clean = clean_text(title)\n",
    "    title_en = translate_to_en(title_clean)\n",
    "    keywords = extract_keywords(title, description)\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    if doi:\n",
    "        queries.append((doi, \"DOI_MATCH\"))\n",
    "\n",
    "    queries.extend([\n",
    "        (title_clean, \"TITLE_ES\"),\n",
    "        (title_en, \"TITLE_EN\"),\n",
    "        (dataset_uri, \"URI_MATCH\")\n",
    "    ])\n",
    "\n",
    "    for kw in keywords:\n",
    "        queries.append((kw, \"KEYWORD\"))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    SOURCES = [\n",
    "        (search_crossref, \"crossref\"),\n",
    "        (search_openalex, \"openalex\"),\n",
    "        (search_zenodo, \"zenodo\"),\n",
    "        (search_cordis, \"cordis\"),\n",
    "        (search_github, \"github\")\n",
    "    ]\n",
    "\n",
    "    for q, match_type in queries:\n",
    "        if not q.strip():\n",
    "            continue\n",
    "\n",
    "        for fn, src in SOURCES:\n",
    "            hits = fn(q)\n",
    "            for h in hits:\n",
    "                h[\"match_type\"] = match_type\n",
    "                h[\"query_used\"] = q\n",
    "                results.append(h)\n",
    "\n",
    "        time.sleep(0.25 + random.random() * 0.25)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "\n",
    "def main():\n",
    "\n",
    "    INPUT = \"Punto1_UKDA_PortalV1 - copia.xlsx\"\n",
    "    OUTPUT = \"Punto2_UK_ReutilizacionV1.xlsx\"\n",
    "\n",
    "    df = pd.read_excel(INPUT)\n",
    "    rows = []\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        dataset_uri = r[\"dataset_uri\"]\n",
    "        title = r[\"title\"]\n",
    "        description = r[\"description\"]\n",
    "        doi = r.get(\"doi\", \"\")\n",
    "\n",
    "        print(f\"ðŸ”Ž Buscando reutilizaciÃ³n: {title}\")\n",
    "\n",
    "        hits = search_reuse(dataset_uri, title, description, doi)\n",
    "\n",
    "        for h in hits:\n",
    "            rows.append({\n",
    "                \"dataset_uri\": dataset_uri,\n",
    "                \"title_dataset\": title,\n",
    "                \"issued\": r[\"issued\"],\n",
    "                \"source\": h.get(\"source\"),\n",
    "                \"external_id\": h.get(\"external_id\"),\n",
    "                \"match_type\": h.get(\"match_type\"),\n",
    "                \"query_used\": h.get(\"query_used\"),\n",
    "                \"source_title\": h.get(\"title\"),\n",
    "                \"url\": h.get(\"url\"),\n",
    "                \"date\": h.get(\"date\")\n",
    "            })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out.to_excel(OUTPUT, index=False)\n",
    "\n",
    "    print(\"\\nâœ” Archivo generado:\", OUTPUT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a8261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
