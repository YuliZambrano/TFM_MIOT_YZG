{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import urllib.parse \n",
    "\n",
    "# ======================\n",
    "# CONFIGURACI√ìN\n",
    "# ======================\n",
    "# NOTA: Aseg√∫rate de que el archivo de entrada est√© en el mismo directorio.\n",
    "\n",
    "INPUT_FILE = \"Punto1_NOMBREARCHIVORESULTADO.xlsx\"                # ARCHIVO RESULTADO DEL SCRIPT 1\n",
    "OUTPUT_FILE = \"Punto2_NOMBREARCHIVORESULTADO.xlsx\"               # ARCHIVO QUE RESULTADO DE ESTE SCRIPT 2\n",
    "\n",
    "GITHUB_TOKEN = None        # AQUI PUEDES USAR TU TOKEN DE GitHub, ponlo aqu√≠ para mejores resultados\n",
    "MAX_RESULTS = 4            # N√∫mero m√°ximo de resultados a obtener por cada fuente (excepto enlaces de b√∫squeda)\n",
    "SLEEP_TIME = 1             # Segundos de espera entre llamadas API para ser cort√©s\n",
    "\n",
    "# ======================\n",
    "# CROSSREF (papers/DOIs)\n",
    "# ======================\n",
    "def search_crossref(query, max_results=MAX_RESULTS):\n",
    "    \"\"\"Busca trabajos acad√©micos en CrossRef usando el t√≠tulo del dataset.\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works?query.bibliographic={query}&rows={max_results}\"\n",
    "        r = requests.get(url, timeout=20)\n",
    "        if r.status_code == 200:\n",
    "            for item in r.json().get(\"message\", {}).get(\"items\", []):\n",
    "                # CrossRef da las partes de la fecha. Usamos la primera parte (a√±o, mes, d√≠a si existe)\n",
    "                date_parts = item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0]\n",
    "                # Formato: [\"YYYY\", \"MM\", \"DD\"] -> \"YYYY-MM-DD\"\n",
    "                date_str = \"-\".join(map(str, date_parts)) if date_parts and date_parts[0] else None \n",
    "\n",
    "                results.append({\n",
    "                    \"source_type\": \"crossref_paper\",\n",
    "                    \"source_title\": item.get(\"title\", [\"Sin t√≠tulo\"])[0],\n",
    "                    \"year\": date_str, # Usamos la fecha completa o YYYY-MM\n",
    "                    \"link\": item.get(\"URL\", \"\")\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\" Error CrossRef: {e}\")\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "#  OPENALEX (Investigaci√≥n global)\n",
    "# ======================\n",
    "def search_openalex(query, max_results=MAX_RESULTS):\n",
    "    \"\"\"Busca trabajos de investigaci√≥n en OpenAlex (la alternativa abierta a Scopus/WoS).\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/works?search={query}&per-page={max_results}\"\n",
    "        r = requests.get(url, timeout=20)\n",
    "        if r.status_code == 200:\n",
    "            for item in r.json().get(\"results\", []):\n",
    "                # OpenAlex devuelve la fecha de publicaci√≥n completa\n",
    "                year_full = item.get(\"publication_date\", \"\") # Formato: YYYY-MM-DD\n",
    "                year = year_full if year_full else None\n",
    "                results.append({\n",
    "                    \"source_type\": \"openalex_work\",\n",
    "                    \"source_title\": item.get(\"title\", \"Sin t√≠tulo\"),\n",
    "                    \"year\": year, # Guardamos la fecha completa YYYY-MM-DD\n",
    "                    \"link\": item.get(\"id\", \"\").replace(\"https://openalex.org/\", \"https://openalex.org/works/\") \n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\" Error OpenAlex: {e}\")\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "# ZENODO (Repositorio de datos y software)\n",
    "# ======================\n",
    "def search_zenodo(query, max_results=MAX_RESULTS):\n",
    "    \"\"\"Busca registros (datasets, software) en Zenodo.\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        url = f\"https://zenodo.org/api/records?q={query}&size={max_results}\"\n",
    "        r = requests.get(url, timeout=20)\n",
    "        if r.status_code == 200:\n",
    "            for item in r.json().get(\"hits\", {}).get(\"hits\", []):\n",
    "                # Zenodo almacena la fecha de publicaci√≥n completa\n",
    "                year_full = item.get(\"metadata\", {}).get(\"publication_date\", \"\") # Formato: YYYY-MM-DD\n",
    "                year = year_full if year_full else None\n",
    "                \n",
    "                results.append({\n",
    "                    \"source_type\": \"zenodo_record\",\n",
    "                    \"source_title\": item.get(\"metadata\", {}).get(\"title\", \"Sin t√≠tulo\"),\n",
    "                    \"year\": year, # Guardamos la fecha completa YYYY-MM-DD\n",
    "                    \"link\": item.get(\"links\", {}).get(\"html\", \"\")\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\" Error Zenodo: {e}\")\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "# GOOGLE SCHOLAR (B√∫squeda de Enlace)\n",
    "# ======================\n",
    "def search_google_scholar(query, max_results=1):\n",
    "    \"\"\"Genera un enlace de b√∫squeda directa en Google Scholar.\"\"\"\n",
    "    q_encoded = urllib.parse.quote(query)\n",
    "    results = [{\n",
    "        \"source_type\": \"google_scholar_search\",\n",
    "        \"source_title\": f\"B√∫squeda en Google Scholar: '{query[:40]}...'\",\n",
    "        \"year\": \"N/A\", # No hay fecha espec√≠fica en un enlace de b√∫squeda\n",
    "        \"link\": f\"https://scholar.google.com/scholar?q={q_encoded}\"\n",
    "    }]\n",
    "    return results[:max_results]\n",
    "\n",
    "# ======================\n",
    "# GITHUB\n",
    "# ======================\n",
    "def search_github(query, max_results=MAX_RESULTS, token=GITHUB_TOKEN):\n",
    "    \"\"\"Busca repositorios en GitHub.\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        url = f\"https://api.github.com/search/repositories?q={query}+in:name,description\"\n",
    "        headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "        if token:\n",
    "            headers[\"Authorization\"] = f\"token {token}\"\n",
    "        r = requests.get(url, headers=headers, timeout=20)\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            for item in r.json().get(\"items\", [])[:max_results]:\n",
    "                # GitHub proporciona el timestamp completo (YYYY-MM-DDTHH:MM:SSZ)\n",
    "                updated_at = item.get(\"updated_at\", \"\")[:10] \n",
    "                results.append({\n",
    "                    \"source_type\": \"github_repo\",\n",
    "                    \"source_title\": item.get(\"full_name\", \"Sin t√≠tulo\"),\n",
    "                    \"year\": updated_at, # Guardamos YYYY-MM-DD\n",
    "                    \"link\": item.get(\"html_url\", \"\")\n",
    "                })\n",
    "        elif r.status_code == 403:\n",
    "            print(\"      Error GitHub (403): L√≠mite de tasa excedido. Usa un GITHUB_TOKEN.\")\n",
    "        else:\n",
    "            print(f\"      Error GitHub ({r.status_code}): {r.text[:50]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error GitHub: {e}\")\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "# KAGGLE, APPS, PROYECTOS (B√∫squeda de Enlace)\n",
    "# ======================\n",
    "# Estas funciones no devuelven fechas de reutilizaci√≥n espec√≠ficas y siguen siendo enlaces gen√©ricos.\n",
    "\n",
    "def search_kaggle(query, max_results=2):\n",
    "    \"\"\"Genera un enlace de b√∫squeda directa en Kaggle.\"\"\"\n",
    "    results = [{\n",
    "        \"source_type\": \"kaggle_search\",\n",
    "        \"source_title\": f\"B√∫squeda en Kaggle: '{query[:50]}'\",\n",
    "        \"year\": \"N/A\", \n",
    "        \"link\": f\"https://www.kaggle.com/search?q={urllib.parse.quote(query)}\"\n",
    "    }]\n",
    "    return results[:max_results]\n",
    "\n",
    "def search_apps(query, max_results=1):\n",
    "    \"\"\"Genera un enlace de b√∫squeda directa en Google Play Store (simulaci√≥n).\"\"\"\n",
    "    q_encoded = urllib.parse.quote(f\"{query} datos andaluc√≠a\")\n",
    "    results = [{\n",
    "        \"source_type\": \"app_store_search\",\n",
    "        \"source_title\": f\"B√∫squeda de Apps: '{query[:40]}...'\",\n",
    "        \"year\": \"N/A\", \n",
    "        \"link\": f\"https://play.google.com/store/search?q={q_encoded}&c=apps\"\n",
    "    }]\n",
    "    return results[:max_results]\n",
    "\n",
    "def search_projects(query, max_results=1):\n",
    "    \"\"\"Genera un enlace de b√∫squeda directa en el portal CORDIS (simulaci√≥n).\"\"\"\n",
    "    q_encoded = urllib.parse.quote(f\"{query} andalucia\")\n",
    "    results = [{\n",
    "        \"source_type\": \"eu_project_search\",\n",
    "        \"source_title\": f\"B√∫squeda de Proyectos UE (CORDIS): '{query[:40]}...'\",\n",
    "        \"year\": \"N/A\", \n",
    "        \"link\": f\"https://cordis.europa.eu/search?q={q_encoded}\"\n",
    "    }]\n",
    "    return results[:max_results]\n",
    "\n",
    "# ======================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ======================\n",
    "def run_reuse_search(input_file, output_file):\n",
    "    # Carga del archivo de entrada\n",
    "    try:\n",
    "        # Asumimos Excel ya que es el formato original\n",
    "        df = pd.read_excel(input_file) \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Error: El archivo de entrada '{input_file}' no se encontr√≥.\")\n",
    "        return pd.DataFrame() \n",
    "\n",
    "    results = []\n",
    "    print(f\" Analizando {len(df)} datasets del archivo {input_file}...\")\n",
    "\n",
    "    possible_title_cols = [\"Titulo\", \"T√≠tulo\", \"title\", \"dataset_title\", \"Nombre\"]\n",
    "    # Columna de fecha de publicaci√≥n en el archivo de entrada\n",
    "    published_date_col = \"Fecha de publicaci√≥n\" \n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"üîé Buscando reutilizaci√≥n\"):\n",
    "        \n",
    "        dataset_id = row.get(\"Dataset_id\", f\"D{idx+1}\") \n",
    "\n",
    "        # --- Buscar t√≠tulo v√°lido ---\n",
    "        dataset_title = None\n",
    "        for col in possible_title_cols:\n",
    "            if col in df.columns and pd.notna(row[col]):\n",
    "                dataset_title = str(row[col])\n",
    "                break\n",
    "        \n",
    "        if not dataset_title:\n",
    "            dataset_title = \"Sin t√≠tulo\"\n",
    "        \n",
    "        # --- Extraer FECHA COMPLETA del dataset original ---\n",
    "        # Si la columna existe y no es nula, toma el valor completo.\n",
    "        # Esto captura el formato YYYY-MM-DDTHH:MM:SS de CKAN\n",
    "        full_published_date = str(row.get(published_date_col, \"\")) if published_date_col in df.columns and pd.notna(row[published_date_col]) else \"\"\n",
    "        \n",
    "        # El a√±o se sigue extrayendo para referencias r√°pidas (solo YYYY)\n",
    "        year_dataset = full_published_date[:4] if full_published_date and len(full_published_date) >= 4 else \"\"\n",
    "\n",
    "        # --- BUSCAR REUTILIZACIONES EN TODAS LAS FUENTES ---\n",
    "        combined = []\n",
    "        combined += search_crossref(dataset_title)\n",
    "        combined += search_openalex(dataset_title)\n",
    "        combined += search_zenodo(dataset_title)\n",
    "        combined += search_google_scholar(dataset_title)\n",
    "        combined += search_github(dataset_title, token=GITHUB_TOKEN)\n",
    "        combined += search_kaggle(dataset_title)\n",
    "        combined += search_apps(dataset_title)\n",
    "        combined += search_projects(dataset_title)\n",
    "\n",
    "        # --- Agregar contexto del dataset ---\n",
    "        for hit in combined:\n",
    "            hit[\"dataset_id\"] = dataset_id\n",
    "            hit[\"dataset_title\"] = dataset_title\n",
    "            # Conservamos el a√±o de publicaci√≥n del dataset (solo YYYY) \n",
    "            hit[\"year_dataset\"] = year_dataset \n",
    "            # GUARDAMOS LA FECHA COMPLETA DEL DATASET ORIGINAL\n",
    "            hit[\"published_date\"] = full_published_date \n",
    "            results.append(hit)\n",
    "\n",
    "        # Pausa para ser cort√©s con las APIs\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # --- Crear DataFrame final ---\n",
    "    df_out = pd.DataFrame(results, columns=[\n",
    "        \"dataset_id\", \"dataset_title\", \"source_type\", \"source_title\",\n",
    "        \"year\", \"year_dataset\", \"published_date\", \"link\"\n",
    "    ])\n",
    "\n",
    "    df_out.to_excel(output_file, index=False)\n",
    "    print(f\"\\n Resultados guardados en {output_file}\")\n",
    "    return df_out\n",
    "\n",
    "# ======================\n",
    "# EJECUTAR\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    df_results = run_reuse_search(INPUT_FILE, OUTPUT_FILE)\n",
    "    if not df_results.empty:\n",
    "        print(\"\\n Muestra de resultados (10 primeras filas):\")\n",
    "        print(df_results.head(10).to_markdown(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
